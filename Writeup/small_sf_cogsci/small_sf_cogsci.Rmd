---
title: "Starting small: Exploring the origins of successor function knowledge in subset knowers"
bibliography: citations.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    <!-- \author{{\large \bf Rose M. Schneider},^1  {\large \bf Ashlie H. Pankonin},^2 {\large \bf Adena Schachner},^1, $\&$ {\large \bf David Barner}^1 -->
    <!-- \\ ^1Department of Psychology, University of California, San Diego \\ -->
    <!-- ^2 School of Speech, Language, and Hearing Sciences, San Diego State University} -->

abstract: >
    Although children in the US learn to accurately count sets by around the age of 3.5 years, they     often do not have a more general understanding of how the count list represents number. In         particular, most children at this age fail to understand that adding 1 item to a set               corresponds to counting up 1 word in the count list, and vice versa. Initially, children have      piecemeal knowledge of this relation, and may know that adding 1 to "four" results in "five",      but may lack generalized knowledge of how this relationship can be applied to any number.          Currently, although we know that this generalized knowledge emerges at around 5.5 or 6 years       of age in US children, we don't know when their earlier piecemeal learning begins, and             therefore when they begin learning how the meanings of number words are related to one another     -- a critical precursor to mathematical reasoning.
    
    <!-- For several years before US children demonstrate a generalized understanding of the successor function -- a recursive function *S* which states that for every natural number *n*, *S(n)* = *n*+1 -- they show striking limits in their ability to implement this logical principle, even for numbers well within their known number range. While recent work has established a link between children's counting proficiency and eventual generalization of the successor function at around 6 years of age, it is unknown when and how children acquire their earlier piecemeal successor knowledge. Here, we explore the timescale and mechanism underlying acquisition of this knowledge in children who have not yet learned the count routine's significance. We find that these children already show evidence of having established localized successor mappings for known number words. Unlike children who understand the importance of the count routine, however, these localized mappings are urelated to their count list knowledge; rather, we find evidence that the origin of this item-based successor knowledge may begin in operations performed over sets stored in working memory. -->

keywords: >
    Number; cognitive development; counting
    
output: cogsci2016::cogsci_paper
# final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, include = FALSE}
rm(list = ls())
require("knitr")
# opts_knit$set(root.dir = "~/Documents/Projects/small_sf/") #this is specific to RMS, change accordingly
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)
library(memisc)
'%!in%' <- function(x,y)!('%in%'(x,y))
```

```{r}
####DATA LOADING AND MANAGEMENT
#load study 1 data
data.raw.study1 <- read.csv("../../Data/small_sf_study1.csv")
data.raw.study2 <- read.csv("../../Data/small_sf_study2.csv")

##Exclude any pilots and copypaste
data.raw.study1 %<>%
  filter(SID != "CopyPasteMe", 
         Exclusion_reason != "Pilot")%>%
  droplevels()%>%
  # dplyr::select(-X, -X.1, -X.2, -X.3, -X.4)%>%
  mutate(Age = as.numeric(as.character(Age)))

data.raw.study2 %<>%
  filter(SID != "CopyPasteMe", 
         Exclusion_reason != "Pilot")%>%
  droplevels()%>%
  # dplyr::select(-X, -X.1, -X.2, -X.3, -X.4)%>%
  mutate(Age = as.numeric(as.character(Age)))

#how many kids pre-exclusions?
#exp.1
pre.excl.exp.1 <- data.raw.study1 %>%
  distinct(SID, Age)%>%
  summarise(n = n())

#exp.2
pre.excl.exp2 <- data.raw.study2 %>%
  distinct(SID, Age)%>%
  summarise(n = n())

#Why are kids excluded?
#exp.1
excl.reason.exp1 <- data.raw.study1 %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude, Exclusion_reason)%>%
  group_by(Exclusion_reason)%>%
  summarise(n = n())
#exp.2
excl.reason.exp2 <- data.raw.study2 %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude, Exclusion_reason)%>%
  group_by(Exclusion_reason)%>%
  summarise(n = n())

#### GLOBAL EXCLUSIONS
#exp.1
data.raw.study1 %<>%
  filter(Exclude != 1)%>%
  droplevels()
#exp.2
data.raw.study2 %<>%
  filter(Exclude != 1)%>%
  droplevels()

#### TRIAL/TASK EXCLUSIONS
#exp.1
data.raw.study1 %<>%
  mutate(Exclude_trial = ifelse(is.na(Exclude_trial), "0", as.character(Exclude_trial)))%>%
  filter(Exclude_trial != "1")

#exp.2
data.raw.study2 %<>%
  mutate(Exclude_trial = ifelse(is.na(Exclude_trial), "0", as.character(Exclude_trial)))%>%
  filter(Exclude_trial != "1")


#how many kids failed training for SF?
#exp.1
fail.sf.training.exp1 <- data.raw.study1 %>%
  filter(Task == "SF", 
         Trial_number == "Training", 
         Correct == "0")%>%
  summarise(n = n())

#exp.2
fail.sf.training.exp2 <- data.raw.study2 %>%
  filter(Task == "SF", 
         Trial_number == "Training", 
         Correct == "0")%>%
  summarise(n = n())

##exclude training for SF
#exp.1
data.raw.study1 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)))%>%
  filter(Trial_number != "Training")

##exclude training for SF
#exp.2
data.raw.study2 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)))%>%
  filter(Trial_number != "Training")

#Exclude training for NN
data.raw.study2 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)), 
         Exclude_NN = ifelse((Task == "Next_number" & Trial_number == "1"), 1, 0))%>%
  filter(Exclude_NN != 1)

## RENAME ABOVE FOR SIMPLICITY
all.data.study1 <- data.raw.study1

all.data.study2 <- data.raw.study2
```

```{r, include = FALSE}
# Data validation
## Checking knower level classifications
#study 1
given.ms <- all.data.study1 %>%
  mutate(Knower_level = ifelse(Knower_level == "CP", 6, as.numeric(as.character(Knower_level)))) %>% #for function below
  filter(Task == "Give_N", 
         !is.na(Task_item))%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

#overall check - an N-knower should have gotten at least .67 mean performance for a given N
check <- given.ms %>%
  filter(Task_item == Knower_level)%>%
  filter(mean < .66)

if(length(check$SID) > 0) {
  print("CHECK STUDY 1 KLs")
}

#study 2
given.ms <- all.data.study2 %>%
  mutate(Knower_level = ifelse(Knower_level == "CP", 6, as.numeric(as.character(Knower_level)))) %>% #for function below
  filter(Task == "Give_N", 
         !is.na(Task_item))%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

#overall check - an N-knower should have gotten at least .67 mean performance for a given N
check <- given.ms %>%
  filter(Task_item == Knower_level)%>%
  filter(mean < .66)

if(length(check$SID) > 0) {
  print("CHECK STUDY 2 KLs")
}
```

```{r}
#change correct to numeric
all.data.study1 %<>%
  mutate(Correct = as.numeric(as.character(Correct)))

all.data.study2 %<>%
  mutate(Correct = as.numeric(as.character(Correct)))
```

```{r}
##add CP_subset
#by CP_subset
#Study 1 
all.data.study1 %<>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", 
                            "Subset"))
#Study 2 
all.data.study2 %<>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", 
                            "Subset"))
```

```{r highest_count}
##add highest count as a column
#study 1
hc.lookup <- all.data.study1 %>%
  filter(Task == "Highest_count")%>%
  dplyr::rename(highest_count = Response)%>%
  dplyr::select(SID, highest_count)

all.data.study1 <- right_join(all.data.study1, hc.lookup, by = "SID")

#HCs are being read in as factors - change to numeric
all.data.study1 %<>%
  mutate(highest_count = as.numeric(as.character(highest_count)))

#one kid didn't do highest count, so we will exclude them from analyses containing hc

#study 2
hc.lookup <- all.data.study2 %>%
  filter(Task == "Highest_count")%>%
  dplyr::rename(highest_count = Response)%>%
  dplyr::select(SID, highest_count)

all.data.study2 <- right_join(all.data.study2, hc.lookup, by = "SID")

#HCs are being read in as factors - change to numeric
all.data.study2 %<>%
  mutate(highest_count = as.numeric(as.character(highest_count)))

#one kid didn't do highest count, so we will exclude them from analyses containing hc
```

# Introduction
<!-- In the course of learning about number, most children come to the profound realization that, for every number they can imagine, there is always a number exactly \emph{one} greater. In the US, children arrive at this form of the \emph{successor function} -- a recursive function stating that the successor of any number \emph{n} is \emph{n}+1 -- around the age of 6, and may even leverage this knowledge to begin reasoning about numerical infinity [@cheung2017;@chu2020]. Prior to this generalization and extension, however, children's knowledge of the successor function is strikingly limited for several years. For example, many children who are otherwise competent counters are only able to reason about how the addition of "+1" affects a set's cardinality for some numbers within their count list, while completely failing to do so for others  [@cheung2017;@davidson2012;@sarnecka2008;@spaepen2018].   -->

Over the course of the first few years of life, children slowly accrue knowledge of the components of symbolic number in isolation, gradually combining and integrating them to arrive at increasingly abstract numerical concepts. Throughout this process, however, their understanding of number is often strikingly limited, indicating either fragile or absent mappings between different pieces of knowledge. A key example of this phenomenon lies in children's protracted discovery that adding 1 to a set always denotes a number word that is exactly 1 greater in the count list -- an understanding that reflects some implicit knowledge of the \emph{successor function}, a logical principle stating that the successor of any number \emph{n} is exactly \emph{n}+1. For 1.5 -- 2 years after children have learned how to use the count routine to generate sets, they demonstrate piecemeal knowledge of the successor function, and can successfully implementing it for only some numbers in their count list [@davidson2012; @cheung2017]. While there is growing evidence that children eventually use their knowledge of the count list to generalize the successor function to \emph{any} number, there has been comparatively little work exploring their early item-based understanding. In the current work, we investigate the origins of children's successor knowledge by exploring the mechanisms through which they begin to acquire these localized mappings.

Children's generalization of the successor function comes several years after an extended process of acquiring the meanings of number words and the count routine. Most US children are able to recite portions of the count list by around 2 years of age [@fuson1988], but it is not until around 2.5 years that they begin to understand the meanings of a subset of those number words [@wynn1990]. These "subset knowers" have acquired meanings for some number words (generally \emph{one}, \emph{two}, and \emph{three}), but have not yet recognized the connection between the cardinalities represented by these number words and the count list which contains them. For example, a "two-knower" might be able to generate sets of \emph{one} and \emph{two}, while having no knowledge of \emph{three} [@wynn1990; @wynn1992]. Around 4 years of age, however, children eventually make this connection, and realize how the last word said while counting indicates the cardinality of a set (i.e. the Cardinal Principle, or CP, @gelman1978).

On some accounts, the process through which children make this induction entails them noticing that the cardinalities associated with their known number words all differ by a quantity of exactly 1, and that this difference is reflected by +1 difference in the count list [@carey2004; @sarnecka2008]. Some evidence for this hypothesis comes from work using Sarnecka and Carey's (2008) paradigm known as the "Unit Task". In this task, an experimenter labels some number of items ("Look! Here are 4 frogs!") and hides them behind an occluder. The experimenter then adds 1 item to this set, and asks, "Are there 5 or 6 frogs now?". To succeed in this task, children need to make an analogical mapping; that is, they must understand that the addition of 1 item to an established cardinality necessitates moving up exactly 1 number in the count list. Compatible with the hypothesis that acquiring the CP necessitates making an induction about an isomorphism between the count list and cardinality, @sarnecka2008 found that while subset knowers were at chance on the Unit Task for sets of 4 and 5, CP-knowers had above-chance performance. 

While @sarnecka2008 found that CP-knowers outperformed subset knowers on the Unit Task, there is strong evidence to suggest that their early Unit Task performance did not show evidence of implicit knowledge of the successor function at all, but instead, may have reflected item-based knowledge with specific numbers. First, not all CP-knowers in @sarnecka2008 were able to pass the Unit Task for 4 and 5 -- numbers well within their known number range. Second, subsequent work has found that knowledge of the count list, rather CP-knowledge in general, is significantly predictive of children's ability to implement the successor function for \emph{any} number, and not simply those within their count list [@davidson2012; @cheung2017].    


- Unit Task success point to CP vs. subset distinction
- Some have argued that CP is necessary because count list knowledge is how you get to SF
- While it's true that CP knowers eventually extend SF on the basis of count list knowledge, it's unclear to what extent this NN - unit mapping is required in the early stages of SF success, given children's limited knowledge; is this true? Is CP knowledge a gatekeeper? >> limited Unit Task success might be because of item-based mappings, and this seems to be the case based on previous work
- Previous work finding that subset knowers fail; is this because they are unable to the necessary set operations required to pass the Unit Task? Or because they were tested on unfamiliar numbers? -- because they didn't focus on numbers which are more familiar to subset knowers, we don't really know when this item-based process begins


many CP-knowers still do not understand how its ordinal structure captures the successor function. For example, in a paradigm frequently used to assess successor function knowledge known as the "Unit Task," many young CP-knowers are unable to determine whether adding +1 to sets of 4, 5, or 6 (numbers well within their count range) results in a set which should be labeled by \emph{n}+1 or \emph{n}+2 [@sarnecka2008;@spaepen2018]. This limited knowledge extends for at least one to two years after learning the CP, suggesting that children's earliest successor mappings may be drawn from their familiarity with specific numbers, as opposed to a generalized principle. There is growing evidence, however, that children may eventually learn how the successor function can be implemented for all numbers through their mastery of the count list. For example, @davidson2012 found that CP-knowers who were able to count higher without error could implement the successor function for all numbers in their count list, while CP-knowers who were less proficient counters were at chance on the Unit Task even for numbers well within their counting range. @cheung2017 subsequently replicated and extended this finding, showing that CP-knowers' counting proficiency and Unit Task performance were significantly correlated with their understanding of numerical infinity. 

Although children eventually go on to extend the successor function to a potential infinity of numbers, it remains unclear why their successor knowledge is so limited in the years before making this induction, and what mechanisms underlie its acquisition. One possibility is that access to the structure of the count list is a necessary precondition for any kind of successor knowledge, however limited. On this hypothesis, only CP-knowers should be able to reason about item-based successor relations, as they are supported by children's ability to map the relationship between set operations and known portions of the count list [@spaepen2018], and that their ability to do so is constrained by their count list mastery. Supporting this view, CP-knowers routinely outperform subset knowers on the Unit Task for numbers within their rote counting range [@davidson2012], and even for sets of 1 and 2 [@sarnecka2008]. Additionally, @spaepen2018 found that CP-knowers' Unit Task performance on sets of 5 and 6 improved with counting training, while subset knowers' performance did not. Thus, it is possible that while the CP is not sufficient to impart generalized successor knowledge, it is at least necessary for acquiring the general mechanism supporting this recursive function. As children's familiarity with the count list grows, their network of localized mappings begins to include more items, and eventually takes the form of a generalized principle.

Another possibility, however, is that learning these item-based relationships may not be dependent upon acquiring the CP and count list knowledge, but may instead be more closely related to familiarity with specific numbers. This second alternative leaves open the possibility that establishing these mappings may begin prior to acquisition of the CP, as subset knowers may be able to track set operations in working memory for known number words ("\emph{one}", "\emph{two}", and "\emph{three}"; @carey2004;@sella2020). Although previous work has found that subset knowers have lower performance on the Unit Task in comparison to CP-knowers [@spaepen2018; @sarnecka2008], this may be because the numbers on which they are being tested generally fall outside their known number range. In fact, when tested on sets of 1 and 2, many subset knowers demonstrate above-chance Unit Task performance [@sarnecka2008]. Additionally, there is some evidence that subset knowers are able to perform the basic set operations in working memory required to succeed on the Unit Task, even without having access to the count list's ordinal structure. Beginning at five months, infants are able to track the addition of 1 item to an occluded set of 1, demonstrating surprise if the occluded set reveals 1, but not 2, items [@wynn1992addition]. @huttenlocher1994 found that children between 2.5 and 4 years were able to track additions of +1 to sets of 1, 2, and 3. Finally, @hughes1981 found that children as young as 3 years were able to succeed on a paradigm similar to the Unit Task for small numbers (1, 2, and 3). 

Taken together, this work suggests that children may, in fact, be in a position to begin reasoning about localized successor relations between known and familiar number words in advance of acquiring the CP and familiarity with the structure of the count list. In the current work, we tested these two hypotheses by exploring subset and CP-knowers’ knowledge of successor relations for very small sets (1 - 5). In Experiment 1, we asked whether subset knowers’ Unit Task performance was significantly above chance within their knower-level, indicating that children begin to establish this item-based network even before acquiring the CP. In Experiment 2 we explored subset knowers’ Unit Task success in relation to their understanding of the count list to determine the mechanism through which they may be able to establish these mappings for known number words. 

# Experiment 1
## Method
The methods and analyses for both Experiments 1 and 2 were pre-registered (https://osf.io/deqzk/?view_only=e49622708a214438bb095f18bdaa8224). All methodological and analytical choices were as pre-registered, unless stated otherwise in-text.

### Participants
```{r exp1.demos}
exp.1.demos.age <- all.data.study1 %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = round(mean(Age, na.rm = TRUE), 2), 
            sd_age = round(sd(Age, na.rm = TRUE), 2))%>%
  mutate(total.n = sum(n))

exp1.demos.sex <- all.data.study1 %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))


### by n-knower level
n.levels.exp1 <- all.data.study1 %>%
  group_by(Knower_level)%>%
  distinct(SID, Age, Knower_level)%>%
  summarise(n = n(),
            mean_age = round(mean(Age, na.rm = TRUE), 2),
            sd_age = round(sd(Age, na.rm = TRUE),2))

```
We recruited `r exp.1.demos.age$total.n[1]` participants between the ages of 2 and 4 years (`r subset(exp1.demos.sex, Sex == "F")$total.n` female, $M_{age} =$ `r round(mean(all.data.study1$Age, na.rm = TRUE), 2)`, $SD_{age} =$ `r round(sd(all.data.study1$Age, na.rm = TRUE),2)`, range $=$ `r round(min(all.data.study1$Age),2)`-`r round(max(all.data.study1$Age),2)` years) from local preschools and the surrounding community in San Diego, US. Fifty-two of these children were classified as subset knowers and `r subset(exp.1.demos.age, CP_subset == "CP")$n` were classified as CP-knowers using the Give-N task. The breakdown of \emph{N}-knower level classifications is shown in Table \ref{tab:demos1}.

\begin{table}[h]
\centering
\begin{tabular}{c c c } 
 \hline
 \emph{N}-knower level & \emph{n} & $M_{age}$ (SD) \\
 \hline
 1-knower & `r n.levels.exp1$n[1]` & `r n.levels.exp1$mean_age[1]` (`r n.levels.exp1$sd_age[1]`)\\
 2-knower & `r n.levels.exp1$n[2]` & `r n.levels.exp1$mean_age[2]` (`r n.levels.exp1$sd_age[2]`)\\ 
 3-knower & `r n.levels.exp1$n[3]` & `r n.levels.exp1$mean_age[3]` (`r n.levels.exp1$sd_age[3]`)\\
 4-knower & `r n.levels.exp1$n[4]` & `r n.levels.exp1$mean_age[4]` (`r n.levels.exp1$sd_age[4]`)\\
 CP-knower & `r n.levels.exp1$n[5]` & `r n.levels.exp1$mean_age[5]` (`r n.levels.exp1$sd_age[5]`)\\
 \hline
\end{tabular}
\caption{Demographics for all participants by knower level classification.}
\label{tab:demos1}
\end{table} 

### Procedure
Children were tested individually in a quiet spot within the classroom or museum or in a room set apart from the classroom. Participants received the tasks in a fixed order (Unit Task, Give-N, and Highest Count).

#### Unit Task 
We assessed children’s successor function knowledge with a modified version of the Unit Task [@sarnecka2008]. The experimenter presented children with an opaque container and some small fish, saying, "This is my fish bowl and these are my fish!" The experimenter then placed between 1 and 5 fish in the bowl, briefly showed them to the child, and said, "Look, I have \emph{N} fish here. \emph{N} fish are swimming in the fish bowl." Children were prevented from counting during this presentation by the experimenter. The experimenter then placed an opaque lid over the container and asked, "How many fish are in the fish bowl?" Children were given two opportunities to pass this memory check; if they failed both, the experimenter told them how many fish there were, and proceeded with the remainder of the trial.

After the memory check, the experimenter said, “Now watch!”, and added one fish to the container before asking, “Are there \emph{N}+1 or \emph{N}+2 fish now?”. Order of the presented alternatives was counterbalanced across trials. If children failed to pick one of the presented alternatives, the experimenter provided the alternatives again verbally and encouraged the child to select one. 

Participants completed a training trial with feedback in which only 1 fish was added to an empty bowl. Once children passed this training trial, they received 10 test trials with neutral feedback. A correct response for a given \emph{N} was \emph{N}+1. “I don’t know” responses were coded as incorrect. Only numeric responses were included in analyses; non-numeric answers were excluded. Trials were classified as either within or outside of a child’s \emph{N}-knower level (e.g., trials with 1 and 2 items were classified as “within” a 2-knower’s range). 

#### Give-N
We used a titrated version of Give-N to assess children's knower level. The experimenter provided children with 10 identical plastic objects (e.g., strawberries, bananas) and a small plastic plate. After familiarizing the child with the game, the experimenter asked them to put \emph{N} items on the plate After the child finished placing a set on the plate, the experimenter asked “Is that \emph{N}? Can you count to make sure?” If the child recognized an error, they were permitted to fix the set. 

If the child succeeded on a requested \emph{N}, the experimenter requested \emph{N}+1 on the next trial, up to six items. If the child failed on a given \emph{N}, the experimenter requested \emph{N}-1 on the next trial. This pattern of titration continued until the child’s knower level could be confidently identified. Children were defined as \emph{N}-knowers if they correctly provided \emph{N} on at least two out of the three trials that \emph{N} was requested, and did not give that \emph{N} more than once when asked for another number. Children who correctly generated sets of 6 at least 2 out of 3 times when requested were classified as CP-knowers. 

#### Highest Count. 
We used the Highest Count task to assess children's counting proficiency. An experimenter introduced the task to the child by saying, “In this game, I want you to count as high as you can! Can you start counting for me with \emph{one}?" 

A child’s Highest Count was defined as the largest number counted to before making an error, or the point at which the child did not know how to continue. The first time a child stopped counting the experimenter prompted them, saying “Do you know what comes next?”. If a child could not continue, the task was ended. Otherwise, the child was able to continue counting until they stopped again. For example, a child who counted from 1 to 9, was prompted, and then continued to count to 19 had a highest count of 19. In comparison, a child who counted from 1 to 9, then skipped to 19 would have a highest count of 9.
```{r model_setup_exp1, include = FALSE}
model.df.study1 <- all.data.study1 %>%
  filter(Task == "SF", 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)))
```

```{r ss_exp_1_chance, include = FALSE}
##DO SS KNOWERS PERFORM SIGNIFICANTLY ABOVE CHANCE
#filter down to subset knowers

subset.chance.model <- glmer(Correct ~ 1 + (1|SID), family = "binomial", 
                               data = subset(model.df.study1, CP_subset == "Subset"))
subset.chance.overall <- summary(subset.chance.model)

```

```{r ss_exp_1_kl, include = FALSE}
##DO SS KNOWERS PERFORM ABOVE CHANCE WITHIN KL
##add to model.df whether a trial was within or outside knower level 
model.df.study1.within <- model.df.study1 %>%
  filter(CP_subset != "CP")%>%
  mutate(within_kl = ifelse(Task_item <= as.numeric(as.character(Knower_level)), "Within", "Outside"))

subset.chance.within.kl <- glmer(Correct ~ 1 + (1|SID), family = "binomial", 
                                 data = subset(model.df.study1.within, within_kl == "Within"))

subset.chance.within.kl.sum <- summary(subset.chance.within.kl)

```

```{r ss_exp_1_hc, include = FALSE}
##DO SS KNOWERS WITH HIGHER HC PERFORM BETTER
##get highest count for each kid, add to model.df 
subset.hc <- glmer(Correct ~ highest_count + (1|SID), 
                   family = "binomial", data = subset(model.df.study1, CP_subset == "Subset"))

#compare to base
hc.ss.anova <- anova(subset.chance.model, subset.hc, test = 'LRT')

##WHAT ABOUT WITHIN THEIR KL
subset.hc.within <- glmer(Correct ~ highest_count + (1|SID), 
                          family = "binomial", data = subset(model.df.study1.within, within_kl == "Within"))

#lrt to compare
anova(subset.chance.within.kl, subset.hc.within, test = 'LRT')
```

## Results and Discussion
```{r exp_1_unit, fig.pos = "tb", fig.width=3.5, fig.height=1.5, fig.align = "center", fig.cap = "Mean Unit Task performance for subset knowers in Exp. 1, grouped by knower level. Error bars represent 95\\% CIs computed by nonparametric bootstrap. Three- and 4-knowers are collapsed together due to small ns."}
#visualization 
all.data.study1 %>%
  filter(Knower_level == "1" | 
           Knower_level == "2" | 
           Knower_level == "3" | 
           Knower_level == "4" )%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Knower_level.combined = ifelse((Knower_level == "4" | Knower_level == "3"), "3- & 4-knowers", as.character(Knower_level)),
         Knower_level.combined = factor(Knower_level.combined, levels= c("1", "2", "3- & 4-knowers", "CP"), 
                               labels = c("1-knowers", "2-knowers", "3- & 4-knowers", "CP-knowers")))%>%
  filter(Task == "SF")%>%
  group_by(Task_item, Knower_level.combined)%>%
  # summarise(mean = mean(Correct, na.rm = TRUE),
  #           n = n(), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Knower_level.combined, group= Knower_level.combined)) +
  geom_point(size = 1, 
             show.legend = FALSE) + 
  geom_line(size = .5, 
            show.legend = FALSE) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey", size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5, 
                show.legend = FALSE) +
  theme_bw(base_size = 7.5) + 
  theme(legend.position = "top", 
        panel.grid = element_blank()) + 
  facet_wrap(~Knower_level.combined, ncol = 3) +
  langcog::scale_color_solarized("Knower level") +
  labs(x = "Starting set size", y = "Mean Unit Task performance")
```
Our primary question in this work was whether subset knowers were able to pass the Unit Task for numbers within their known number range, despite not yet having acquired the CP. To test this, we built a null generalized linear mixed effects model with subset-knower data predicting Unit Task performance, with a random effect of subject.\footnote{All mixed effects models were fit in \texttt{R} using the \texttt{lme4} package. The model specification for the null model was: \texttt{Correct $\sim$ 1 + ( 1 | subject)}} First, collapsing over both known and unknown trials, this model indicated that subset knowers’ performance on this task was significantly different from chance (Wald \emph{Z} $=$ `r round(subset.chance.overall$coefficients[3], 2)`, $p =$ .0007), with generally accurate performance ($\beta$ $=$ `r round(subset.chance.overall$coefficients[1], 2)`), as shown in Figure \ref{fig:exp_1_cp_subset}. 

Next, we tested whether subset knowers demonstrated better Unit Task performance for familiar numbers. We tested this by rerunning our null generalized linear mixed effects model using data only from trials that were within subset knowers’ known range. Once again, we found that performance was significantly different than chance (Wald \emph{Z} $=$ `r round(subset.chance.within.kl.sum$coefficients[3], 2)`, $p$ < .0001), with higher accuracy for items which were within subset knowers' known range ($\beta$ = `r round(subset.chance.within.kl.sum$coefficients[1], 2)`). 
```{r exp_1_t_tests, include = FALSE}
subset.ms <- all.data.study1 %>%
  filter(Task == "SF", 
         CP_subset == "Subset")%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))

##1-knowers
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS

##2-knowers
two_knower.t.1 <- t.test(subset(subset.ms, Knower_level == "2" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##Significant, t(17) = 6.65, p < .0001
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS

#3-knowers
three_t.test.1 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##Significant, t(13) = 8.83, p < .0001
three_t.test.2 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##Significant, t(13) = 3.80, p = .002
three_t.test.3 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##Marginal, t(13) = 2.12, p = .05
t.test(subset(subset.ms, Knower_level == "3" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "3" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS
```

Figure \ref{fig:exp_1_unit} shows \emph{N}-knowers' performance on each queried item. Follow-up analyses indicated that subset knowers' performance Unit Task performance roughly tracked with their Knower level, such that 2-knowers were above chance for sets of 1 (\emph{t}(17) = `r round(two_knower.t.1$statistic, 2)`, $p$ < .0001), and 3-knowers were above chance for sets of 1 (\emph{t}(13) = `r round(three_t.test.1$statistic, 2)`, $p$ < .0001), 2 (\emph{t}(13) = `r round(three_t.test.2$statistic, 2)`, $p$ $=$ .002), and marginally for sets of 3 (\emph{t}(13) = `r round(three_t.test.3$statistic, 2)`, $p =$ .05). One-knowers were not significantly above chance for any set size (all \emph{p}s > .05).

Finally, we tested whether subset knowers' Unit Task performance was related to their counting proficiency, which has been linked to CP-knowers' successor knowledge. We built another generalized linear mixed effects model predicting subset knowers' Unit Task performance from their highest count (centered and scaled), with a random effect of participant. This model included both known and unknown set sizes. A Likelihood Ratio Test between this model and the null model indicated that the addition of the highest count term did not significantly improve the fit of the model for either overall performance ($\chi^2$ $=$ `r round(hc.ss.anova$Chisq[2],2)`, $p$ $=$ 0.45, or for numbers falling within a child’s known number range ($\chi^2$ $=$ 0.65, p = 0.42). Thus, it does not appear that subset knowers’ Unit Task performance is related to their knowledge of the count list. 

```{r cp_subset, include = FALSE}
###TESTING FOR DIFFERENCE BETWEEN CP AND SUBSET KNOWERS
model.cp.study1 <- all.data.study1 %>%
  filter(Task == "SF", 
         !is.na(highest_count), 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)), 
          age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)))

#construct base model with age
cp.subset.base <- glmer(Correct ~ age.c + (1|SID), 
                        family = "binomial", data = model.cp.study1)

#add knower level - does this explain additional variance?
cp.subset.kl <- glmer(Correct ~ CP_subset + age.c + (1|SID), 
                      family = "binomial", data = model.cp.study1)

#compare - does KL improve the fit of the base model? 
anova(cp.subset.base, cp.subset.kl, test = 'LRT')
cp.subset.kl.sum <- summary(cp.subset.kl)
```

```{r cp_chance, include = FALSE}
###are cp-knowers performing above chance for 4 and 5? 
cp.ms <- all.data.study1 %>%
  filter(Task == "SF", 
         CP_subset == "CP")%>%
  group_by(SID, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  ungroup()

cp.unit.t.test.4 <- t.test(subset(cp.ms, Task_item == 4)$mean, mu = .5, var.equal = TRUE)#nope
cp.unit.t.test.5 <- t.test(subset(cp.ms, Task_item == 5)$mean, mu = .5, var.equal = TRUE)#nope
```

We additionally tested whether, consistent with previous work [@sarnecka2008; @spaepen2018], we found significantly more accurate Unit Task performance for CP-knowers in comparison to subset knowers. To test this, we built another generalized linear mixed effects model predicting Unit Task performance from CP-knower status and age with a random effect of participant.\footnote{Model specification: \texttt{Correct $\sim$ CP-knower status + Age + ( 1 | subject)}} Compatible with prior work, this model indicated that subset knowers were significantly less accurate on the Unit Task in comparison to CP-knowers (`r round(cp.subset.kl.sum$coefficients[2], 2)`, $p =$ .02;  Figure \ref{fig:exp_1_cp_subset}). Although CP-knowers had significantly greater Unit Task performance in comparison to subset knowers, their performance provides more evidence that  acquiring the CP does not guarantee implicit successor knowledge [@davidson2012; @cheung2017; @spaepen2018], as CP-knowers did not perform significantly different than chance for sets of either 4 (`r round(cp.unit.t.test.4$statistic, 2)`, $p =$  `r round(cp.unit.t.test.4$p.value, 2)`) or 5 (`r round(cp.unit.t.test.5$statistic, 2)`, $p =$  `r round(cp.unit.t.test.5$p.value, 2)`). 

```{r exp_1_cp_subset, fig.width= 2.7, fig.height=1.8, fig.align = "center", fig.cap = "Mean Unit Task performance for CP- and subset knowers in Exp. 1. Error bars represent 95\\% CIs computed by nonparametric bootstrap."}

cp.subset.pal <- c("#CAB2D6", "#6A3D9A" )
all.data.study1 %>%
    filter(Task == "SF")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP"), 
                            labels = c("Subset-knower", "CP-knower")))%>%
  group_by(Task_item, CP_subset)%>%
   # summarise(mean = mean(Correct, na.rm = TRUE), 
   #          n = n(), 
   #          sd = sd(Correct, na.rm = TRUE), 
   #          se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = CP_subset, group= CP_subset)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey", size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size = 7.5) + 
  theme(panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(.25, 1)) + 
  scale_color_manual(values = cp.subset.pal) +
  labs(x = "Starting set size", y = "Mean Unit Task performance", 
       color = "Knower level")
```

# Experiment 2
In Experiment 1, we found that subset knowers performed significantly above chance on the Unit Task for a range of small numbers (1-5), and that they had significantly greater accuracy for items that were within their Knower level. In Experiment 2, we explored two potential mechanisms underlying this success. The first is that subset knowers, like CP-knowers, could be using an analogical mapping drawn from the portion of the count list containing these known numbers. The second is that, as suggested by the null relationship between counting proficiency and Unit Task performance in Experiment 1, that subset knowers could succeed on the Unit Task by deploying item-based mappings between known number words and small sets stored in working memory. To explore these two hypotheses, we added the Next Number task to probe children's count list knowledge, and compared subset knowers' performance between these two tasks. 

## Method
### Participants
```{r exp2.demos}
exp.2.demos.age <- all.data.study2 %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = round(mean(Age, na.rm = TRUE), 2), 
            sd_age = round(sd(Age, na.rm = TRUE), 2))%>%
  mutate(total.n = sum(n))

exp2.demos.sex <- all.data.study2 %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))


### by n-knower level
n.levels.exp2 <- all.data.study2 %>%
  group_by(Knower_level)%>%
  distinct(SID, Age, Knower_level)%>%
  summarise(n = n(),
            mean_age = round(mean(Age, na.rm = TRUE), 2),
            sd_age = round(sd(Age, na.rm = TRUE),2))

```

We recruited `r exp.2.demos.age$total.n[1]` participants between the ages of 2 and 4 years (`r subset(exp2.demos.sex, Sex == "F")$total.n` female, $M_{age} =$ `r round(mean(all.data.study2$Age, na.rm = TRUE), 2)`, $SD_{age} =$ `r round(sd(all.data.study2$Age, na.rm = TRUE),2)`, range $=$ `r round(min(all.data.study2$Age),2)`-`r round(max(all.data.study2$Age),2)` years) from local preschools and the surrounding community in San Diego, US. Forty-five of these children were classified as subset knowers and `r subset(exp.2.demos.age, CP_subset == "CP")$n` were classified as CP-knowers using the Give-N task. The breakdown of \emph{N}-knower level classifications is shown in Table \ref{tab:demos2}.

\begin{table}[h]
\centering
\begin{tabular}{c c c } 
 \hline
 \emph{N}-knower level & \emph{n} & $M_{age}$ (SD) \\
 \hline
 1-knower & `r n.levels.exp2$n[1]` & `r n.levels.exp2$mean_age[1]` (`r n.levels.exp2$sd_age[1]`)\\
 2-knower & `r n.levels.exp2$n[2]` & `r n.levels.exp2$mean_age[2]` (`r n.levels.exp2$sd_age[2]`)\\ 
 3-knower & `r n.levels.exp2$n[3]` & `r n.levels.exp2$mean_age[3]` (`r n.levels.exp2$sd_age[3]`)\\
 4-knower & `r n.levels.exp2$n[4]` & `r n.levels.exp2$mean_age[4]` (`r n.levels.exp2$sd_age[4]`)\\
 5-knower & `r n.levels.exp2$n[5]` & `r n.levels.exp2$mean_age[5]`\\
 CP-knower & `r n.levels.exp2$n[6]` & `r n.levels.exp2$mean_age[6]` (`r n.levels.exp2$sd_age[6]`)\\
 \hline
\end{tabular}
\caption{Demographics for all participants by knower level classification.}
\label{tab:demos2}
\end{table} 

### Procedure
Stimuli and methods were identical to Experiment 1 with two exceptions. First, we added the Next Number task to test whether subset knowers were drawing on their knowledge of the count list’s structure to succeed on the Unit Task. This task requires children to count up from an arbitrary point in the count list in response to a prompt (e.g., "\emph{N}, what comes next?"). The Next Number task included the same numbers as those tested in the Unit Task (1-5). Children received a training trial with 'one', during which they received feedback, and then received 10 additional trials. Items were present in a pseudo-randomized order, and each number was queried twice. As in Experiment 1, participants received the tasks in a fixed order (Unit Task, Next Number, Give-N, and Highest Count). 

Second, we also controlled for the possibility that children could succeed on the Unit Task through subitizing the final set when the lid was removed to add fish during each trial. In Experiment 2, we used a lid with a small slot through which the experimenter could insert this fish, thus preventing children from observing the final set. 

## Results and Discussion
```{r exp_2_model, include= FALSE}
##This is for Next number AND SF analyses
model.df.study2 <- all.data.study2 %>%
  filter(Task == "SF" | 
           Task == "Next_number", 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)))
```

```{r nn_vs_unit, include = FALSE}
###COMPARING NN VS UNIT FOR SS OVERALL

#make base
nn.v.sf.base <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset"))
#add task
nn.v.sf.task <- glmer(Correct ~ Task + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset"))
#compare
anova(nn.v.sf.base, nn.v.sf.task, test= 'LRT')#significant
subset.nn.unit <- summary(nn.v.sf.task)
```

```{r nn_unit_within, include = FALSE}
###TESTING WHETHER NN AND UNIT DIFFER FOR ITEMS WITHIN KL
#add a term indicating whether item is within/outside knower level
model.df.study2.within <- model.df.study2 %>%
  mutate(within_kl = ifelse(Task_item <= as.numeric(as.character(Knower_level)), "Within", "Outside"), 
         within_kl = factor(within_kl))

#make base
within.nn.v.sf.base <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & 
                                                         within_kl == "Within"))
#add task
within.nn.v.sf.task <- glmer(Correct ~ Task + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & 
                                                         within_kl == "Within"))
#compare
anova(within.nn.v.sf.base, within.nn.v.sf.task, test= 'LRT') # significant
nn.unit.subset.within <- summary(within.nn.v.sf.task)

```

```{r ss_unit_exp2, include = FALSE}
##SS UNIT CHANCE TEST
subset.sf.chance <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset" & Task == "SF"))

subset.unit.exp2 <- summary(subset.sf.chance)
```

```{r ss_unit_exp2_within, include = FALSE}
##SS UNIT WITHIN CHANCE TEST
subset.sf.chance.within <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & Task == "SF" 
                                                         & within_kl == "Within"))
subset.unit.known.exp2 <- summary(subset.sf.chance.within)
```

First, we replicated our finding that subset knowers demonstrate above-chance Unit Task performance overall (Wald \emph{Z} $=$ `r round(subset.unit.exp2$coefficients[3], 2)`, $p =$ .012), and that this effect was driven by numbers within their known number range (Wald \emph{Z} $=$ `r round(subset.unit.exp2$coefficients[3], 2)`, $p =$ .0002). Thus, it does not appear that subset knowers' above-chance performance in Experiment 1 was due to them subitizing the final set.

We next turned to our primary question in Experiment 2, which was whether subset knowers' above-chance performance on the Unit Task was drawn from their knowledge of the count list. We tested this by comparing subset knowers' performance on the Unit and Next Number tasks with a generalized linear mixed effects model predicting task performance (Unit or Next Number) with a random effect of participant.\footnote{Model specification: \texttt{Correct $\sim$ Task + ( 1 | subject)}}  This model indicated that subset knowers' Next Number performance was significantly lower than their Unit Task performance, both overall ($\beta =$ -`r round(subset.nn.unit$coefficients[2,1], 2)`, $p$ < .0001), and for numbers within their known number range ($\beta =$ `r round(nn.unit.subset.within$coefficients[2,1], 2)`, $p$ < .0001; Figure \ref{fig:exp_2_unit_nn}). This difference in performance between the Unit and Next Number tasks suggest that subset knowers are not drawing upon their knowledge of the count list to reason about successor relations for small numbers, but may instead be mappings between small set representations stored in working memory and known number words.
```{r exp_2_unit_nn, fig.pos = "t", fig.width=3.5, fig.height=1.9, fig.align = "center", fig.cap = "Mean Unit Task and Next Number performance for subset knowers in Exp. 2, grouped by knower level. Error bars represent 95\\% CIs computed by nonparametric bootstrap. Three-, 4-, and 5- knowers are collapsed together due to small ns."}
task.pal <- c("#cb4b16", "#2aa198")

all.data.study2 %>%
   filter(Task == "SF" | 
           Task == "Next_number")%>%
  filter(Knower_level == "1" | 
           Knower_level == "2" | 
           Knower_level == "3" | 
           Knower_level == "4" | 
           Knower_level == "5")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Knower_level.combined = ifelse((Knower_level == "4" | Knower_level == "3" | Knower_level == "5"), "3-, 4-, & 5-knowers", as.character(Knower_level)),
         Knower_level.combined = factor(Knower_level.combined, levels= c("1", "2", "3-, 4-, & 5-knowers"), 
                               labels = c("1-knowers", "2-knowers", "3-, 4-, & 5-knowers")), 
         Task = factor(Task, levels = c("Next_number", "SF"), 
                labels = c("Next Number", "Unit Task")))%>%
  group_by(Knower_level.combined, Task, Task_item)%>%
  # summarise(n = n(), 
  #           mean = mean(Correct, na.rm = TRUE), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Task, group= Task)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size = 7.5) + 
  theme(legend.position = "top", 
        panel.grid = element_blank(), 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  facet_wrap(~Knower_level.combined) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(0, 1)) + 
  scale_color_manual(values = task.pal)+ 
  labs(x = "Number queried", y = "Mean task performance")
```

```{r, cp_nn_compare, include = FALSE}
####UNIT NN COMPARISON####
model.cp.study2 <- model.df.study2 %>%
  filter(!is.na(highest_count))
cp.nn.compare <- glmer(Correct ~ Task + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, CP_subset == "CP"))
cp.nn.sum <- summary(cp.nn.compare)

##UNIT##
cp.unit.base <- glmer(Correct ~ age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF"))
#now add CP-subset for comparison
cp.unit.compare <- glmer(Correct ~ factor(CP_subset, levels = c("Subset", "CP")) + age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF"))
#compare
anova(cp.unit.base, cp.unit.compare, test= 'LRT')#significant
cp.unit.2 <- summary(cp.unit.compare)

#highest count with unit
cp.unit.hc.base <- glmer(Correct ~ age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF" & CP_subset == "CP"))
cp.unit.hc. <- glmer(Correct ~ highest_count.c + age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF" & CP_subset == "CP"))
#compare
anova(cp.unit.hc.base, cp.unit.hc., test = 'LRT')

####Next Number####
#make base
cp.subset.base.nn <- glmer(Correct ~ age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))


#add cp_subset
cp.subset.kl.nn <- glmer(Correct ~ factor(CP_subset, levels = c("Subset", "CP")) + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))
#compare
anova(cp.subset.base.nn, cp.subset.kl.nn, test = 'LRT')
cp.nn.2 <- summary(cp.subset.kl.nn)

#add highest_count
cp.subset.hc.nn <- glmer(Correct ~ highest_count.c + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number")) ##NB model is failing to converge, need to check if this is a big deal
with(cp.subset.hc.nn@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #we're okay
#compare
anova(cp.subset.base.nn, cp.subset.hc.nn, test = 'LRT') #hc significantly adds to base model
summary(cp.subset.hc.nn)

##Add KL to HC model
nn.plus.kl <-  glmer(Correct ~ CP_subset + highest_count.c + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))
#compare
anova(cp.subset.hc.nn, nn.plus.kl, test = 'LRT') #cp and NN
summary(nn.plus.kl)
```

```{r cp_chance.2, include = FALSE}
###are cp-knowers performing above chance for 4 and 5? 
cp.ms <- all.data.study2 %>%
  filter(Task == "SF", 
         CP_subset == "CP")%>%
  group_by(SID, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  ungroup()

cp.unit.t.test.4.2 <- t.test(subset(cp.ms, Task_item == 4)$mean, mu = .5, var.equal = TRUE)#nope
cp.unit.t.test.5.2 <- t.test(subset(cp.ms, Task_item == 5)$mean, mu = .5, var.equal = TRUE)#nope
```

Next, we tested whether there was evidence that CP-knowers, who have greater familiarity with the count list and its structure, were using this knowledge to succeed on the Unit Task. First, we replicated our previous finding that CP-knowers performed significantly better than subset knowers on the Unit Task ($\beta =$ `r round(cp.unit.2$coefficients[2,1], 2)`, $p =$ .002). As in Experiment 1, however, we again found that CP-knowers were at chance on the Unit Task for sets of 4 (`r round(cp.unit.t.test.4.2$statistic, 2)`, $p =$  `r round(cp.unit.t.test.4.2$p.value, 2)`) and 5 (`r round(cp.unit.t.test.5.2$statistic, 2)`, $p =$  `r round(cp.unit.t.test.5.2$p.value, 2)`).

Finally, we found that CP-knowers outperformed subset knowers on the Next Number  ($\beta =$ `r round(cp.nn.2$coefficients[2,1], 2)`, $p <$ .0001), and critically, that there was no difference in CP-knowers' Unit Task and Next Number performance ($\beta =$ `r round(cp.nn.sum$coefficients[2,1], 2)`, $p =$ .21), as shown in Figure \ref{fig:exp_2_cp_sub}. Thus, we did find some evidence that CP-knowers who have access to the ordinal structure of the count list may be able to recruit this knowledge when reasoning about successor relations for these small numbers. Consistent with other work showing that the CP is not sufficient for Unit Task success, however, we found that CP-knowers were at chance for sets of 4 and 5.
```{r exp_2_cp_sub, fig.width=3.5, fig.height=1.9, fig.align = "center", fig.cap = "Mean Unit Task and Next Number performance for CP- and subset knowers in Exp. 2. Error bars represent 95\\% CIs computed by nonparametric bootstrap."}
all.data.study2 %>%
   filter(Task == "SF" | 
           Task == "Next_number")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Task = factor(Task, levels = c("Next_number", "SF"), 
                labels = c("Next Number", "Unit Task")), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP"), 
                            labels = c("Subset-knower", "CP-knower")))%>%
  group_by(CP_subset, Task, Task_item)%>%
  # summarise(n = n(), 
  #           mean = mean(Correct, na.rm = TRUE), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Task, group= Task)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size =7.5) + 
  theme(
        panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  facet_wrap(~CP_subset) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(0, 1)) + 
  scale_color_manual(values = task.pal)+ 
  labs(x = "Number queried", y = "Mean task performance")
```

# General Discussion
For several years before children are able to explicitly state that it is possible to add 1 to any number, they seem to have fragmented knowledge of the successor function, successfully implementing it for some familiar numbers, yet failing for others [@davidson2012; @cheung2017; @spaepen2018]. How does this item-based knowledge arise, and what is its relationship to a more generalized understanding of this property of the natural numbers? We explored these questions in the current work in two ways. In Experiment 1 we asked whether demonstrating this item-based knowledge occurs only after CP acquisition, or whether subset knowers' previous failures could be explained by the fact that they are typically tested on numbers that fall outside their known number range. In Experiment 2, we explored the possible mechanisms underlying the origins of the successor function, testing whether such knowledge relied on knowledge of the count list, or could be explained by mappings between known number words and sets stored in working memory. 

In both Experiments, we found that subset knowers performed significantly better than chance on the Unit Task when tested on items that fell within their known number range. Further, we found evidence that subset knowers’ Unit Task performance tracked with their number word knowledge, improving as they accumulated more number word meanings. In Experiment 2, we found that subset knowers' Unit Task success was not related to their count list knowledge; rather, it appears that these children were likely maintaining and updating these small sets using the Parallel Individuation system [@carey2004] and, if they had the appropriate number word for the set, mapping it onto the final cardinality. These findings are consistent with hypothesis that the Parallel Individuation system may form a foundational role in helping children acquire the number words \emph{one}, \emph{two}, and \emph{three} [@carey2004;@carey2019]. These results further suggest that children are capable of using these memory-based representations to perform the necessary set operations to succeed in the Unit Task without the benefit of the count list's structure. Thus, these results indicate that children may begin to acquire item-based successor mappings quite early in numerical development, and that these mappings may be initially independent of count list knowledge.

Consistent with other work, however, we found that children’s successor knowledge increased after they acquired the CP, with CP-knowers out-performing subset knowers on the Unit Task in both Experiments. Further, we found some evidence that CP-knowers, who have some understanding of the count list and its relationship to cardinality, may actively recruit this knowledge in establishing these item-based successor mappings; in Experiment 2, CP-knowers' Next Number performance was not significantly different than their Unit Task performance. While recent work has suggested that children eventually use the structure of the count list to make a full induction about the successor function [@cheung2017;@chu2020], our results indicate that even young CP-knowers may be starting to recognize the count list’s significance in solving these tasks. CP-knowers' performance on sets that fell outside the limits of working memory (4 and 5) and could only have been solved by using the count list indicates that this knowledge is not necessarily entailed by acquiring the CP, however. 

What role do these early item-based successor mappings play in leading children to the eventual conclusion that every number has a successor? One possibility is that these early item-based mappings may play an important role in establishing the basic set-operation mechanisms needed to implement the successor function. Our results suggest that this process may begin even before children learn the significance of the count routine, and is constrained by their number word knowledge. After children become CP-knowers, and their lexicon of known number words increases, the set of numbers for which they are able to reason about successor relations may increase, causing children to assert that it is possible to add +1 to some, but not all numbers. These localized networks may lay the ground for a larger induction about the successor function, as proposed by @davidson2012. This induction may be made when children learn that number words are recursively generated — and that the item-based successor function that they apply to some numbers actually extends to all numbers [@cheung2017;@chu2020;@schneider2020]. 

Taken together, our results suggest that children begin establishing the general conceptual framework which grounds successor function knowledge quite early in development, and is not dependent upon knowing the CP. Prior to learning the significance of the count routine and its relationship to cardinality, children use the numerical tools at their disposal, such as the Parallel Individuation system, to begin establishing the basic set-operations of the successor function. As children acquire more numerical tools and knowledge, they are incorporated into this limited successor knowledge, possibly forming the foundation for a larger induction about this logical principle. 
<!-- From intro: While much remains to be discovered about children's successor function acquisition, a potential framework for this process has begun to emerge. It is possible that children first begin learning about the successor function by establishing a network of localized successor relations, as proposed by @davidson2012. As children master more of the the count list, these new numbers may be gradually incorporated into this network, incrementally providing evidence that adding +1 to cardinalities results in a +1 change in the count list. Mastering the recursive structure of the count list may allow children to extend this item-based knowledge, supporting an induction about how the successor function is implicated in the recursive generation of number words [@schneider2020;@chu2020]. -->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
