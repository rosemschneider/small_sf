---
title: "Starting small: Exploring the origins of successor function knowledge in subset knowers"
bibliography: citations.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    <!-- \author{{\large \bf Rose M. Schneider},^1  {\large \bf Ashlie H. Pankonin},^2 {\large \bf Adena Schachner},^1, $\&$ {\large \bf David Barner}^1 -->
    <!-- \\ ^1Department of Psychology, University of California, San Diego \\ -->
    <!-- ^2 School of Speech, Language, and Hearing Sciences, San Diego State University} -->

abstract: >
    For several years before US children demonstrate a generalized understanding of the successor function -- a recursive function *S* which states that for every natural number *n*, *S(n)* = *n*+1 -- they show striking limits in their ability to implement this logical principle, even for numbers well within their known number range. While previous work has established a link between children's counting proficiency and generalization of the successor function at 6 years of age, it is unknown when and how children acquire this piecemeal successor knowledge. Here, we explore the timescale and mechanism underlying this knowledge in children who have not yet learned the count routine's significance, or its relationship to cardinality. We find that these children already exhibit evidence of establishing these localized successor mappings, but that this knowledge is unrelated to their knowledge of the count list; rather, we find evidence that the origins of children's successor function knowledge may begin in operations performed over sets stored in working memory.

keywords: >
    Number; cognitive development; counting
    
output: cogsci2016::cogsci_paper
# final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, include = FALSE}
rm(list = ls())
require("knitr")
# opts_knit$set(root.dir = "~/Documents/Projects/small_sf/") #this is specific to RMS, change accordingly
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)
library(memisc)
'%!in%' <- function(x,y)!('%in%'(x,y))
```

```{r}
####DATA LOADING AND MANAGEMENT
#load study 1 data
data.raw.study1 <- read.csv("../../Data/small_sf_study1.csv")
data.raw.study2 <- read.csv("../../Data/small_sf_study2.csv")

##Exclude any pilots and copypaste
data.raw.study1 %<>%
  filter(SID != "CopyPasteMe", 
         Exclusion_reason != "Pilot")%>%
  droplevels()%>%
  # dplyr::select(-X, -X.1, -X.2, -X.3, -X.4)%>%
  mutate(Age = as.numeric(as.character(Age)))

data.raw.study2 %<>%
  filter(SID != "CopyPasteMe", 
         Exclusion_reason != "Pilot")%>%
  droplevels()%>%
  # dplyr::select(-X, -X.1, -X.2, -X.3, -X.4)%>%
  mutate(Age = as.numeric(as.character(Age)))

#how many kids pre-exclusions?
#exp.1
pre.excl.exp.1 <- data.raw.study1 %>%
  distinct(SID, Age)%>%
  summarise(n = n())

#exp.2
pre.excl.exp2 <- data.raw.study2 %>%
  distinct(SID, Age)%>%
  summarise(n = n())

#Why are kids excluded?
#exp.1
excl.reason.exp1 <- data.raw.study1 %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude, Exclusion_reason)%>%
  group_by(Exclusion_reason)%>%
  summarise(n = n())
#exp.2
excl.reason.exp2 <- data.raw.study2 %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude, Exclusion_reason)%>%
  group_by(Exclusion_reason)%>%
  summarise(n = n())

#### GLOBAL EXCLUSIONS
#exp.1
data.raw.study1 %<>%
  filter(Exclude != 1)%>%
  droplevels()
#exp.2
data.raw.study2 %<>%
  filter(Exclude != 1)%>%
  droplevels()

#### TRIAL/TASK EXCLUSIONS
#exp.1
data.raw.study1 %<>%
  mutate(Exclude_trial = ifelse(is.na(Exclude_trial), "0", as.character(Exclude_trial)))%>%
  filter(Exclude_trial != "1")

#exp.2
data.raw.study2 %<>%
  mutate(Exclude_trial = ifelse(is.na(Exclude_trial), "0", as.character(Exclude_trial)))%>%
  filter(Exclude_trial != "1")


#how many kids failed training for SF?
#exp.1
fail.sf.training.exp1 <- data.raw.study1 %>%
  filter(Task == "SF", 
         Trial_number == "Training", 
         Correct == "0")%>%
  summarise(n = n())

#exp.2
fail.sf.training.exp2 <- data.raw.study2 %>%
  filter(Task == "SF", 
         Trial_number == "Training", 
         Correct == "0")%>%
  summarise(n = n())

##exclude training for SF
#exp.1
data.raw.study1 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)))%>%
  filter(Trial_number != "Training")

##exclude training for SF
#exp.2
data.raw.study2 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)))%>%
  filter(Trial_number != "Training")

#Exclude training for NN
data.raw.study2 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)), 
         Exclude_NN = ifelse((Task == "Next_number" & Trial_number == "1"), 1, 0))%>%
  filter(Exclude_NN != 1)

## RENAME ABOVE FOR SIMPLICITY
all.data.study1 <- data.raw.study1

all.data.study2 <- data.raw.study2
```

```{r, include = FALSE}
# Data validation
## Checking knower level classifications
#study 1
given.ms <- all.data.study1 %>%
  mutate(Knower_level = ifelse(Knower_level == "CP", 6, as.numeric(as.character(Knower_level)))) %>% #for function below
  filter(Task == "Give_N", 
         !is.na(Task_item))%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

#overall check - an N-knower should have gotten at least .67 mean performance for a given N
check <- given.ms %>%
  filter(Task_item == Knower_level)%>%
  filter(mean < .66)

if(length(check$SID) > 0) {
  print("CHECK STUDY 1 KLs")
}

#study 2
given.ms <- all.data.study2 %>%
  mutate(Knower_level = ifelse(Knower_level == "CP", 6, as.numeric(as.character(Knower_level)))) %>% #for function below
  filter(Task == "Give_N", 
         !is.na(Task_item))%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

#overall check - an N-knower should have gotten at least .67 mean performance for a given N
check <- given.ms %>%
  filter(Task_item == Knower_level)%>%
  filter(mean < .66)

if(length(check$SID) > 0) {
  print("CHECK STUDY 2 KLs")
}
```

```{r}
#change correct to numeric
all.data.study1 %<>%
  mutate(Correct = as.numeric(as.character(Correct)))

all.data.study2 %<>%
  mutate(Correct = as.numeric(as.character(Correct)))
```

```{r}
##add CP_subset
#by CP_subset
#Study 1 
all.data.study1 %<>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", 
                            "Subset"))
#Study 2 
all.data.study2 %<>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", 
                            "Subset"))
```

```{r highest_count}
##add highest count as a column
#study 1
hc.lookup <- all.data.study1 %>%
  filter(Task == "Highest_count")%>%
  dplyr::rename(highest_count = Response)%>%
  dplyr::select(SID, highest_count)

all.data.study1 <- right_join(all.data.study1, hc.lookup, by = "SID")

#HCs are being read in as factors - change to numeric
all.data.study1 %<>%
  mutate(highest_count = as.numeric(as.character(highest_count)))

#one kid didn't do highest count, so we will exclude them from analyses containing hc

#study 2
hc.lookup <- all.data.study2 %>%
  filter(Task == "Highest_count")%>%
  dplyr::rename(highest_count = Response)%>%
  dplyr::select(SID, highest_count)

all.data.study2 <- right_join(all.data.study2, hc.lookup, by = "SID")

#HCs are being read in as factors - change to numeric
all.data.study2 %<>%
  mutate(highest_count = as.numeric(as.character(highest_count)))

#one kid didn't do highest count, so we will exclude them from analyses containing hc
```

# Introduction
In the course of learning about number, most children come to a profound realization: that for every number they can imagine, there is always a number exactly \emph{one} greater. Although US children seem to arrive at this understanding of the \emph{successor function} around the age of 6, and its acquisition is correlated with developing notions of numerical infinity [@cheung2017; @chu2020], knowledge of this logical principle is strikingly limited for several years. For instance, many children who are otherwise competent counters can only reason about how adding +1 to a set is reflected in the ordinal structure of the count list for a subset of known numbers  [@cheung2017;@davidson2012;@sarnecka2008;@spaepen2018]. While there is a growing body of research on children's generalization and extension of the successor function, there has been comparatively little work exploring this early item-based knowledge. In the current work, we investigate the origins of children's successor knowledge by exploring the mechanisms through which children acquire these localized successor relations, even before they fully understand the counting system. 

Similar to the successor function, the majority of children's other numerical knowledge is initially limited and item-based. While most US children are able to recite portions of the count list by around 2 years of age [@fuson1988], it is not until around 2.5 years that they begin to understand the meaning of a subset of those number words [@wynn1990]. Until around the age of about 4 years, most children learn the first several number words in a sequential manner, but without understanding the connection between the cardinalities represented by number words and the count list which contains them. These "subset knowers" have acquired meanings for some number words, but not others; for example, a "two-knower" might be able to generate sets of \emph{one} and \emph{two}, while having no knowledge of \emph{three} [@wynn1990; @wynn1992]. Around 4 years of age, however, children eventually make this connection, and realize how the last word said while counting indicates the cardinality of a set (i.e. the Cardinal Principle/ CP, @gelman1978). 

Despite understanding how to use the count list to determine sets' cardinalities, many CP-knowers still do not understand how the count list's ordinal structure captures the successor function. For example, many young CP-knowers are unable to judge whether the result of adding 1 item to sets of 4, 5, or 6 (numbers well within their count range) should be labeled by \emph{n}+1 or \emph{n}+2 in a paradigm known as the "Unit Task" [@sarnecka2008;@spaepen2018]. This limited knowledge extends for at least one to two years after acquiring the CP, and there is growing evidence implicating children's knowledge of the generative nature of the count list in learning how the successor function can be implemented for \emph{any} number. For example, @davidson2012 found that although CP knowers with lower counting proficiency were at chance on the Unit Task, even for numbers that were within their count range, higher counters were above chance for all known numbers, a finding subsequently replicated by @cheung2017. Finally, @chu2020 recently found that children who demonstrate knowledge of these productive counting rules are more likely to state that numbers are endless.  

While there is growing evidence that acquiring generalized successor knowledge is linked to counting proficiency, and occurs after CP acquisition, it is unclear how children may begin to establish their network of localized successor relations. One possibility is that acquiring the CP may be a necessary precondition for reasoning about successor relations [@spaepen2018; @sarnecka2008]. On this hypothesis, the CP may act as a "gatekeeper" for even this limited successor knowledge because children must have access to the ordinal structure of the count list to reason about set operations between known number words. Supporting this view, CP-knowers routinely outperform subset knowers on the Unit Task for numbers within their rote counting range [@davidson2012], and even for sets of 1 and 2 [@sarnecka2008]. Additionally, @spaepen2018 found that CP-knowers' Unit Task performance on sets of 5 and 6 improved with counting training, while subset knowers' performance did not. Thus, it is possible that while the CP is not sufficient to impart generalized successor knowledge, it is at least necessary for reasoning about item-based successor relations. 

Another possibility, however, is that learning these item-based relationships may not be dependent upon acquiring the CP, but may instead be more closely related to their knowledge of specific numbers. While previous work has found that subset knowers perform worse on the Unit Task in comparison to CP-knowers [@spaepen2018; @sarnecka2008], it is possible that this may be because the numbers on which they are being tested fall outside their known number range. In fact, when tested on sets of 1 and 2, many subset knowers demonstrate above-chance Unit Task performance [@sarnecka2008]. Additionally, there is some evidence that subset knowers are able to perform the basic set operations required to succeed on the Unit Task, even without having access to the count lists' ordinal structure. Beginning at five months, infants are able to track the addition of 1 item to an occluded set of 1, demonstrating surprise if the occluded set reveals 1, but not 2, items [@wynn1992addition]. At around 10 months, children are able to track the addition of graham crackers to a bucket to discriminate between sets of 2 and 3 [@feigenson2002]. @huttenlocher1994 found that children between 2.5 and 4 years were able to track additions of 1 to sets of 1, 2, and 3. Finally, @hughes1981 found that children as young as 3 years were able to succeed on a paradigm similar to the Unit Task for small numbers (1, 2, and 3). 

Taken together, this work suggests that children may, in fact, be in a position to begin reasoning about localized successor relations between known and familiar number words in advance of acquiring the CP. In the current work, we tested these two hypotheses by exploring subset and CP-knowers’ knowledge of successor relations for very small sets (1 - 5). In Experiment 1, we asked whether subset knowers’ Unit Task performance was significantly above chance within their knower-level. In Experiment 2 we explored subset knowers’ successor knowledge in relation to their understanding of the count list in an effort to determine the mechanism through which they may be able to reason about successor relations for known number words. 

# Experiment 1
## Method
The methods and analyses for both Experiments 1 and 2 were pre-registered (https://osf.io/deqzk/?view_only=e49622708a214438bb095f18bdaa8224). All methodological and analytical choices were as pre-registered, unless stated otherwise in-text.

### Participants
```{r exp1.demos}
exp.1.demos.age <- all.data.study1 %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = round(mean(Age, na.rm = TRUE), 2), 
            sd_age = round(sd(Age, na.rm = TRUE), 2))%>%
  mutate(total.n = sum(n))

exp1.demos.sex <- all.data.study1 %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))


### by n-knower level
n.levels.exp1 <- all.data.study1 %>%
  group_by(Knower_level)%>%
  distinct(SID, Age, Knower_level)%>%
  summarise(n = n(),
            mean_age = round(mean(Age, na.rm = TRUE), 2),
            sd_age = round(sd(Age, na.rm = TRUE),2))

```
We recruited `r exp.1.demos.age$total.n[1]` participants between the ages of 2 and 4 years (`r subset(exp1.demos.sex, Sex == "F")$total.n` female, $M_{age} =$ `r round(mean(all.data.study1$Age, na.rm = TRUE), 2)`, $SD_{age} =$ `r round(sd(all.data.study1$Age, na.rm = TRUE),2)`, range $=$ `r round(min(all.data.study1$Age),2)`-`r round(max(all.data.study1$Age),2)` years) from local preschools and the surrounding community in San Diego, US. `r subset(exp.1.demos.age, CP_subset == "Subset")$n` of these children were classified as subset knowers and `r subset(exp.1.demos.age, CP_subset == "CP")$n` were classified as CP-knowers using the Give-N task. The breakdown of \emph{N}-knower level classifications is shown in \ref{tab:demos1}.

\begin{table}[h]
\centering
\begin{tabular}{c c c } 
 \hline
 \emph{N}-knower level & \emph{n} & $M_{age}$ (SD) \\
 \hline
 1-knower & `r n.levels.exp1$n[1]` & `r n.levels.exp1$mean_age[1]` (`r n.levels.exp1$sd_age[1]`)\\
 2-knower & `r n.levels.exp1$n[2]` & `r n.levels.exp1$mean_age[2]` (`r n.levels.exp1$sd_age[2]`)\\ 
 3-knower & `r n.levels.exp1$n[3]` & `r n.levels.exp1$mean_age[3]` (`r n.levels.exp1$sd_age[3]`)\\
 4-knower & `r n.levels.exp1$n[4]` & `r n.levels.exp1$mean_age[4]` (`r n.levels.exp1$sd_age[4]`)\\
 CP-knower & `r n.levels.exp1$n[5]` & `r n.levels.exp1$mean_age[5]` (`r n.levels.exp1$sd_age[5]`)\\
 \hline
\end{tabular}
\caption{Demographics for all participants by knower level classification.}
\label{tab:demos1}
\end{table} 

### Procedure
Children were tested individually in a quiet spot within the classroom or museum or in a room set apart from the classroom. Participants received the tasks in a fixed order (Unit Task, Give-N, and Highest Count).

#### Unit Task 
We assessed children’s successor function knowledge with a modified version of the Unit Task [@sarnecka2008]. The experimenter presented children with an opaque container and some small fish, saying, "This is my fish bowl and these are my fish!" The experimenter then placed between 1 and 5 fish in the bowl, briefly showed them to the child, and said, "Look, I have \emph{N} fish here. \emph{N} fish are swimming in the fish bowl." Children were prevented from counting during this presentation by the experimenter. The experimenter then placed an opaque lid over the container and asked, "How many fish are in the fish bowl?" Children were given two opportunities to pass this memory check; if they failed both, the experimenter told them how many fish there were, and proceeded with the remainder of the trial.

After the memory check, the experimenter said, “Now watch!”, and added one fish to the container before asking, “Are there \emph{N}+1 or \emph{N}+2 fish now?”. Order of the presented alternatives was counterbalanced across trials. If children failed to pick one of the presented alternatives, the experimenter provided the alternatives again verbally and encouraged the child to select one. 

Participants completed a training trial with feedback in which only 1 fish was added to an empty bowl. Once children passed this training trial, they received 10 test trials with neutral feedback. A correct response for a given \emph{N} was \emph{N}+1. “I don’t know” responses were coded as incorrect. Only numeric responses were included in analyses; non-numeric answers were excluded. Trials were classified as either within or outside of a child’s \emph{N}-knower level (e.g., trials with 1 and 2 items were classified as “within” a 2-knower’s range). 

#### Give-N
We used a titrated version of Give-N to assess children's knower level. The experimenter provided children with 10 identical plastic objects (e.g., strawberries, bananas) and a small plastic plate. After familiarizing the child with the game, the experimenter asked them to put \emph{N} items on the plate After the child finished placing a set on the plate, the experimenter asked “Is that \emph{N}? Can you count to make sure?” If the child recognized an error, they were permitted to fix the set. 

If the child succeeded on a requested \emph{N}, the experimenter requested \emph{N}+1 on the next trial, up to six items. If the child failed on a given \emph{N}, the experimenter requested \emph{N}-1 on the next trial. This pattern of titration continued until the child’s knower level could be confidently identified. Children were defined as \emph{N}-knowers if they correctly provided \emph{N} on at least two out of the three trials that \emph{N} was requested, and did not give that \emph{N} more than once when asked for another number. Children who correctly generated sets of 6 at least 2 out of 3 times when requested were classified as CP-knowers. 

#### Highest Count. 
We used the Highest Count task to assess children's counting proficiency. An experimenter introduced the task to the child by saying, “In this game, I want you to count as high as you can! Can you start counting for me with \emph{one}?" 

A child’s Highest Count was defined as the largest number counted to before making an error, or the point at which the child did not know how to continue. The first time a child stopped counting the experimenter prompted them, saying “Do you know what comes next?”. If a child could not continue, the task was ended. Otherwise, the child was able to continue counting until they stopped again. For example, a child who counted from 1 to 9, was prompted, and then continued to count to 19 had a highest count of 19. In comparison, a child who counted from 1 to 9, then skipped to 19 would have a highest count of 9.
```{r model_setup_exp1, include = FALSE}
model.df.study1 <- all.data.study1 %>%
  filter(Task == "SF", 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)))
```

```{r ss_exp_1_chance, include = FALSE}
##DO SS KNOWERS PERFORM SIGNIFICANTLY ABOVE CHANCE
#filter down to subset knowers

subset.chance.model <- glmer(Correct ~ 1 + (1|SID), family = "binomial", 
                               data = subset(model.df.study1, CP_subset == "Subset"))
subset.chance.overall <- summary(subset.chance.model)

```

```{r ss_exp_1_kl, include = FALSE}
##DO SS KNOWERS PERFORM ABOVE CHANCE WITHIN KL
##add to model.df whether a trial was within or outside knower level 
model.df.study1.within <- model.df.study1 %>%
  filter(CP_subset != "CP")%>%
  mutate(within_kl = ifelse(Task_item <= as.numeric(as.character(Knower_level)), "Within", "Outside"))

subset.chance.within.kl <- glmer(Correct ~ 1 + (1|SID), family = "binomial", 
                                 data = subset(model.df.study1.within, within_kl == "Within"))

subset.chance.within.kl.sum <- summary(subset.chance.within.kl)

```

```{r ss_exp_1_hc, include = FALSE}
##DO SS KNOWERS WITH HIGHER HC PERFORM BETTER
##get highest count for each kid, add to model.df 
subset.hc <- glmer(Correct ~ highest_count + (1|SID), 
                   family = "binomial", data = subset(model.df.study1, CP_subset == "Subset"))

#compare to base
hc.ss.anova <- anova(subset.chance.model, subset.hc, test = 'LRT')

##WHAT ABOUT WITHIN THEIR KL
subset.hc.within <- glmer(Correct ~ highest_count + (1|SID), 
                          family = "binomial", data = subset(model.df.study1.within, within_kl == "Within"))

#lrt to compare
anova(subset.chance.within.kl, subset.hc.within, test = 'LRT')
```

## Results and Discussion
Our primary question in this work was whether subset knowers were able to pass the Unit Task for numbers within their known number range, despite not yet having acquired the CP. To test this, we built a null generalized linear mixed effects model with subset-knower data predicting Unit Task performance, with a random effect of subject.\footnote{All mixed effects models were fit in \texttt{R} using the \texttt{lme4} package. The model specification for the null model was: \texttt{Correct $\sim$ 1 + ( 1 | subject)}} First, we collapsed over both known and unknown trials, and found this model indicated that subset knowers’ performance on this task was significantly different from chance (Wald \emph{Z} $=$ `r round(subset.chance.overall$coefficients[3], 2)`, $p =$ .0007), with generally accurate performance ($\beta$ $=$ `r round(subset.chance.overall$coefficients[1], 2)`), as shown in \ref{fig:exp_1_cp_subset}. 

Next, we tested whether subset knowers demonstrated better Unit Task performance from familiar numbers. We tested this by re-running our null generalized linear mixed effects model using data only from trials that were within subset knowers’ known range; for example,  two-knowers' known number trials would include sets of 1 and 2. Once again, we found that performance was significantly different than chance (Wald \emph{Z} $=$ `r round(subset.chance.within.kl.sum$coefficients[3], 2)`, $p$ < .0001), with higher accuracy for items which were within subset knowers' known range ($\beta$ = `r round(subset.chance.within.kl.sum$coefficients[1], 2)`). 
```{r exp_1_t_tests, include = FALSE}
subset.ms <- all.data.study1 %>%
  filter(Task == "SF", 
         CP_subset == "Subset")%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))

##1-knowers
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS

##2-knowers
two_knower.t.1 <- t.test(subset(subset.ms, Knower_level == "2" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##Significant, t(17) = 6.65, p < .0001
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS

#3-knowers
three_t.test.1 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##Significant, t(13) = 8.83, p < .0001
three_t.test.2 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##Significant, t(13) = 3.80, p = .002
three_t.test.3 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##Marginal, t(13) = 2.12, p = .05
t.test(subset(subset.ms, Knower_level == "3" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "3" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS
```

\ref{fig:exp_1_unit} shows \emph{N}-knowers' performance on each queried item. Follow-up analyses testing N-knowers’ mean performance for individual task items against chance ($\mu$ = .5), indicated that performance for small numbers was related to children's knowledge of number words. These follow-up analyses revealed that children's Unit Task performance roughly tracked with their Knower level, such that 2-knowers were above chance for sets of 1 (\emph{t}(17) = `r round(two_knower.t.1$statistic, 2)`, $p$ < .0001); and 3-knowers with sets of 1 (\emph{t}(13) = `r round(three_t.test.1$statistic, 2)`, $p$ < .0001), 2 (\emph{t}(13) = `r round(three_t.test.2$statistic, 2)`, $p$ $=$ .002), and marginally for sets of 3 (\emph{t}(13) = `r round(three_t.test.3$statistic, 2)`, $p =$ .05). One-knowers were not significantly above chance for any set size (all \emph{p}s < .05).

Finally, we tested whether subset knowers' Unit Task performance was related to their counting knowledge. We built another generalized linear mixed effects model predicting subset knowers' Unit Task performance from their highest count (centered and scaled), with a random effect of participant. This model included both known and unknown set sizes. A Likelihood Ratio Test between this model and the null model indicated that the addition of the highest count term did not significantly improve the fit of the model for either overall performance ($\chi^2$ $=$ `r round(hc.ss.anova$Chisq[2],2)`, $p$ $=$ 0.45, or for numbers falling within a child’s known number range ($\chi^2$ $=$ 0.65, p = 0.42). Thus, it does not appear that subset knowers’ Unit Task performance is related to their knowledge of the count list. 
```{r exp_1_unit, fig.pos = "b", fig.width=3.5, fig.height=1.5, fig.align = "center", fig.cap = "Mean Unit Task performance for subset knowers in Exp. 1, grouped by knower level. Error bars represent 95\\% CIs computed by nonparametric bootstrap. Three- and 4-knowers are collapsed together due to small ns."}
#visualization 
all.data.study1 %>%
  filter(Knower_level == "1" | 
           Knower_level == "2" | 
           Knower_level == "3" | 
           Knower_level == "4" )%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Knower_level.combined = ifelse((Knower_level == "4" | Knower_level == "3"), "3- & 4-knowers", as.character(Knower_level)),
         Knower_level.combined = factor(Knower_level.combined, levels= c("1", "2", "3- & 4-knowers", "CP"), 
                               labels = c("1-knowers", "2-knowers", "3- & 4-knowers", "CP-knowers")))%>%
  filter(Task == "SF")%>%
  group_by(Task_item, Knower_level.combined)%>%
  # summarise(mean = mean(Correct, na.rm = TRUE),
  #           n = n(), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Knower_level.combined, group= Knower_level.combined)) +
  geom_point(size = 1, 
             show.legend = FALSE) + 
  geom_line(size = .5, 
            show.legend = FALSE) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey", size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5, 
                show.legend = FALSE) +
  theme_bw(base_size = 7.5) + 
  theme(legend.position = "top", 
        panel.grid = element_blank()) + 
  facet_wrap(~Knower_level.combined, ncol = 3) +
  langcog::scale_color_solarized("Knower level") +
  labs(x = "Starting set size", y = "Mean Unit Task performance")
```

```{r cp_subset, include = FALSE}
###TESTING FOR DIFFERENCE BETWEEN CP AND SUBSET KNOWERS
model.cp.study1 <- all.data.study1 %>%
  filter(Task == "SF", 
         !is.na(highest_count), 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)), 
          age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)))

#construct base model with age
cp.subset.base <- glmer(Correct ~ age.c + (1|SID), 
                        family = "binomial", data = model.cp.study1)

#add knower level - does this explain additional variance?
cp.subset.kl <- glmer(Correct ~ CP_subset + age.c + (1|SID), 
                      family = "binomial", data = model.cp.study1)

#compare - does KL improve the fit of the base model? 
anova(cp.subset.base, cp.subset.kl, test = 'LRT')
cp.subset.kl.sum <- summary(cp.subset.kl)
```

```{r cp_chance, include = FALSE}
###are cp-knowers performing above chance for 4 and 5? 
cp.ms <- all.data.study1 %>%
  filter(Task == "SF", 
         CP_subset == "CP")%>%
  group_by(SID, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  ungroup()

cp.unit.t.test.4 <- t.test(subset(cp.ms, Task_item == 4)$mean, mu = .5, var.equal = TRUE)#nope
cp.unit.t.test.5 <- t.test(subset(cp.ms, Task_item == 5)$mean, mu = .5, var.equal = TRUE)#nope
```

We additionally tested whether, consistent with previous work [@sarnecka2008; @spaepen2018], we found significantly more accurate Unit Task performance for CP-knowers in comparison to subset knowers. To test this, we built another generalized linear mixed effects model predicting Unit Task performance from CP-knower status and age with a random effect of participant.\footnote{Model specification: \texttt{Correct $\sim$ CP-knower status +Age + ( 1 | subject)}} Compatible with prior work, this model indicated that subset knowers were significantly less accurate in the Unit Task in comparison to CP-knowers (`r round(cp.subset.kl.sum$coefficients[2], 2)`, $p =$ .02;  \ref{fig:exp_1_cp_subset}). Although CP-knowers had significantly greater Unit Task performance in comparison to subset knowers, their performance was compatible with the hypothesis that even acquiring the CP does not guarantee implicit successor knowledge [@davidson2012; @cheung2017; @spaepen2018], as CP-knowers did not perform significantly different than chance for sets of either 4 (`r round(cp.unit.t.test.4$statistic, 2)`, $p =$  `r round(cp.unit.t.test.4$p.value, 2)`) or 5 (`r round(cp.unit.t.test.5$statistic, 2)`, $p =$  `r round(cp.unit.t.test.5$p.value, 2)`). 

```{r exp_1_cp_subset, fig.width= 2.7, fig.height=1.8, fig.align = "center", fig.cap = "Mean Unit Task performance for CP- and subset knowers in Exp. 1. Error bars represent 95\\% CIs computed by nonparametric bootstrap."}

cp.subset.pal <- c("#CAB2D6", "#6A3D9A" )
all.data.study1 %>%
    filter(Task == "SF")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP"), 
                            labels = c("Subset-knower", "CP-knower")))%>%
  group_by(Task_item, CP_subset)%>%
   # summarise(mean = mean(Correct, na.rm = TRUE), 
   #          n = n(), 
   #          sd = sd(Correct, na.rm = TRUE), 
   #          se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = CP_subset, group= CP_subset)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey", size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size = 7.5) + 
  theme(panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(.25, 1)) + 
  scale_color_manual(values = cp.subset.pal) +
  labs(x = "Starting set size", y = "Mean Unit Task performance", 
       color = "Knower level")
```

# Experiment 2
In Experiment 1, we found that subset knowers performed significantly above chance on the Unit Task for a range of small numbers (1-5), and that they had significantly greater accuracy for items that were within their Knower level. In Experiment 2, we explored two potential mechanisms underlying this success. The first is that subset knowers, like CP-knowers, could be using an analogical mapping drawn from the portion of the count list containing these known numbers, while the second is that they could be succeeding on the Unit Task by deploying item-based mappings between known number words and small sets stored in working memory. To explore these two hypotheses, we added the Next Number task to probe children's count list knowledge, and compared subset knowers' performance between these two tasks. If subset knowers are using their count list knowledge to perform the Unit Task, we should find a significant relationship between these two tasks. 

## Method
### Participants
```{r exp2.demos}
exp.2.demos.age <- all.data.study2 %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = round(mean(Age, na.rm = TRUE), 2), 
            sd_age = round(sd(Age, na.rm = TRUE), 2))%>%
  mutate(total.n = sum(n))

exp2.demos.sex <- all.data.study2 %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))


### by n-knower level
n.levels.exp2 <- all.data.study2 %>%
  group_by(Knower_level)%>%
  distinct(SID, Age, Knower_level)%>%
  summarise(n = n(),
            mean_age = round(mean(Age, na.rm = TRUE), 2),
            sd_age = round(sd(Age, na.rm = TRUE),2))

```

We recruited `r exp.2.demos.age$total.n[1]` participants between the ages of 2 and 4 years (`r subset(exp2.demos.sex, Sex == "F")$total.n` female, $M_{age} =$ `r round(mean(all.data.study2$Age, na.rm = TRUE), 2)`, $SD_{age} =$ `r round(sd(all.data.study2$Age, na.rm = TRUE),2)`, range $=$ `r round(min(all.data.study2$Age),2)`-`r round(max(all.data.study2$Age),2)` years) from local preschools and the surrounding community in San Diego, US. Forty-five of these children were classified as subset knowers and `r subset(exp.2.demos.age, CP_subset == "CP")$n` were classified as CP-knowers using the Give-N task. The breakdown of \emph{N}-knower level classifications is shown in \ref{tab:demos2}.

\begin{table}[h]
\centering
\begin{tabular}{c c c } 
 \hline
 \emph{N}-knower level & \emph{n} & $M_{age}$ (SD) \\
 \hline
 1-knower & `r n.levels.exp2$n[1]` & `r n.levels.exp2$mean_age[1]` (`r n.levels.exp2$sd_age[1]`)\\
 2-knower & `r n.levels.exp2$n[2]` & `r n.levels.exp2$mean_age[2]` (`r n.levels.exp2$sd_age[2]`)\\ 
 3-knower & `r n.levels.exp2$n[3]` & `r n.levels.exp2$mean_age[3]` (`r n.levels.exp2$sd_age[3]`)\\
 4-knower & `r n.levels.exp2$n[4]` & `r n.levels.exp2$mean_age[4]` (`r n.levels.exp2$sd_age[4]`)\\
 5-knower & `r n.levels.exp2$n[5]` & `r n.levels.exp2$mean_age[5]`\\
 CP-knower & `r n.levels.exp2$n[6]` & `r n.levels.exp2$mean_age[6]` (`r n.levels.exp2$sd_age[6]`)\\
 \hline
\end{tabular}
\caption{Demographics for all participants by knower level classification.}
\label{tab:demos2}
\end{table} 

### Procedure
Stimuli and methods were identical to Experiment 1 with two exceptions. First, we added the Next Number task to test whether subset knowers were drawing on their knowledge of the count list’s structure to succeed on the Unit Task. This task requires children to count up from an arbitrary point in the count list in response to a prompt (e.g., "\emph{N}, what comes next?"). The Next Number task included the same numbers as those tested in the Unit Task (1-5). Children received a training trial with 'one', during which they received feedback, and then received 10 additional trials. Items were present in a pseudo-randomized order, and each number was queried twice. As in Experiment 1, participants received the tasks in a fixed order (Unit Task, Next Number, Give-N, and Highest Count). 

Second, we also controlled for the possibility that children could succeed on the Unit Task through subitizing the final set when the lid was removed to add the fish during each trial. In Experiment 2, we used a lid with a small slot through which the experimenter could insert this fish, thus preventing children from observing the final set. 

## Results and Discussion
```{r exp_2_model, include= FALSE}
##This is for Next number AND SF analyses
model.df.study2 <- all.data.study2 %>%
  filter(Task == "SF" | 
           Task == "Next_number", 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)))
```

```{r nn_vs_unit, include = FALSE}
###COMPARING NN VS UNIT FOR SS OVERALL

#make base
nn.v.sf.base <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset"))
#add task
nn.v.sf.task <- glmer(Correct ~ Task + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset"))
#compare
anova(nn.v.sf.base, nn.v.sf.task, test= 'LRT')#significant
subset.nn.unit <- summary(nn.v.sf.task)
```

```{r nn_unit_within, include = FALSE}
###TESTING WHETHER NN AND UNIT DIFFER FOR ITEMS WITHIN KL
#add a term indicating whether item is within/outside knower level
model.df.study2.within <- model.df.study2 %>%
  mutate(within_kl = ifelse(Task_item <= as.numeric(as.character(Knower_level)), "Within", "Outside"), 
         within_kl = factor(within_kl))

#make base
within.nn.v.sf.base <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & 
                                                         within_kl == "Within"))
#add task
within.nn.v.sf.task <- glmer(Correct ~ Task + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & 
                                                         within_kl == "Within"))
#compare
anova(within.nn.v.sf.base, within.nn.v.sf.task, test= 'LRT') # significant
nn.unit.subset.within <- summary(within.nn.v.sf.task)

```

```{r ss_unit_exp2, include = FALSE}
##SS UNIT CHANCE TEST
subset.sf.chance <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset" & Task == "SF"))

subset.unit.exp2 <- summary(subset.sf.chance)
```

```{r ss_unit_exp2_within, include = FALSE}
##SS UNIT WITHIN CHANCE TEST
subset.sf.chance.within <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & Task == "SF" 
                                                         & within_kl == "Within"))
subset.unit.known.exp2 <- summary(subset.sf.chance.within)
```

First, we replicated our finding that subset knowers demonstrate above-chance Unit Task performance overall (Wald \emph{Z} $=$ `r round(subset.unit.exp2$coefficients[3], 2)`, $p =$ .012), and that this effect was driven by numbers within their known number range (Wald \emph{Z} $=$ `r round(subset.unit.exp2$coefficients[3], 2)`, $p =$ .0002). Thus, it does not appear that subset knowers' above-chance performance in Experiment 1 was due to them subitizing the final set.

We next turned to our primary question, which was whether subset knowers' above-chance performance on the Unit Task was related to their knowledge of the count list. We tested this by comparing subset knowers' performance on the Unit and Next Number tasks with a generalized linear mixed effects model predicting task performance (Unit or Next Number) with a random effect of participant.\footnote{Model specification: \texttt{Correct $\sim$ Task + ( 1 | subject)}}  This model indicated that subset knowers' Next Number performance was significantly lower than their Unit Task performance, both overall ($\beta =$ -`r round(subset.nn.unit$coefficients[2,1], 2)`, $p$ < .0001), and for numbers within their known number range ($\beta =$ `r round(nn.unit.subset.within$coefficients[2,1], 2)`, $p$ < .0001; Figure \ref{fig:exp_2_unit.nn}). This difference in performance between the Unit and Next Number tasks suggest that subset knowers are not drawing upon their knowledge of the count list to reason about successor relations for small numbers, but may instead be mappings between small set representations stored in working memory and known number words.
```{r exp_2_unit_nn, fig.pos = "b", fig.width=3.5, fig.height=1.9, fig.align = "center", fig.cap = "Mean Unit Task and Next Number performance for subset knowers in Exp. 2, grouped by knower level. Error bars represent 95\\% CIs computed by nonparametric bootstrap. Three-, 4-, and 5- knowers are collapsed together due to small ns."}
task.pal <- c("#cb4b16", "#2aa198")

all.data.study2 %>%
   filter(Task == "SF" | 
           Task == "Next_number")%>%
  filter(Knower_level == "1" | 
           Knower_level == "2" | 
           Knower_level == "3" | 
           Knower_level == "4" | 
           Knower_level == "5")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Knower_level.combined = ifelse((Knower_level == "4" | Knower_level == "3" | Knower_level == "5"), "3-, 4-, & 5-knowers", as.character(Knower_level)),
         Knower_level.combined = factor(Knower_level.combined, levels= c("1", "2", "3-, 4-, & 5-knowers"), 
                               labels = c("1-knowers", "2-knowers", "3-, 4-, & 5-knowers")), 
         Task = factor(Task, levels = c("Next_number", "SF"), 
                labels = c("Next Number", "Unit Task")))%>%
  group_by(Knower_level.combined, Task, Task_item)%>%
  # summarise(n = n(), 
  #           mean = mean(Correct, na.rm = TRUE), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Task, group= Task)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size = 7.5) + 
  theme(legend.position = "top", 
        panel.grid = element_blank(), 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  facet_wrap(~Knower_level.combined) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(0, 1)) + 
  scale_color_manual(values = task.pal)+ 
  labs(x = "Number queried", y = "Mean task performance")
```

```{r, cp_nn_compare, include = FALSE}
####UNIT NN COMPARISON####
model.cp.study2 <- model.df.study2 %>%
  filter(!is.na(highest_count))
cp.nn.compare <- glmer(Correct ~ Task + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, CP_subset == "CP"))
cp.nn.sum <- summary(cp.nn.compare)

##UNIT##
cp.unit.base <- glmer(Correct ~ age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF"))
#now add CP-subset for comparison
cp.unit.compare <- glmer(Correct ~ factor(CP_subset, levels = c("Subset", "CP")) + age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF"))
#compare
anova(cp.unit.base, cp.unit.compare, test= 'LRT')#significant
cp.unit.2 <- summary(cp.unit.compare)

#highest count with unit
cp.unit.hc.base <- glmer(Correct ~ age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF" & CP_subset == "CP"))
cp.unit.hc. <- glmer(Correct ~ highest_count.c + age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF" & CP_subset == "CP"))
#compare
anova(cp.unit.hc.base, cp.unit.hc., test = 'LRT')

####Next Number####
#make base
cp.subset.base.nn <- glmer(Correct ~ age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))


#add cp_subset
cp.subset.kl.nn <- glmer(Correct ~ factor(CP_subset, levels = c("Subset", "CP")) + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))
#compare
anova(cp.subset.base.nn, cp.subset.kl.nn, test = 'LRT')
cp.nn.2 <- summary(cp.subset.kl.nn)

#add highest_count
cp.subset.hc.nn <- glmer(Correct ~ highest_count.c + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number")) ##NB model is failing to converge, need to check if this is a big deal
with(cp.subset.hc.nn@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #we're okay
#compare
anova(cp.subset.base.nn, cp.subset.hc.nn, test = 'LRT') #hc significantly adds to base model
summary(cp.subset.hc.nn)

##Add KL to HC model
nn.plus.kl <-  glmer(Correct ~ CP_subset + highest_count.c + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))
#compare
anova(cp.subset.hc.nn, nn.plus.kl, test = 'LRT') #cp and NN
summary(nn.plus.kl)
```

```{r cp_chance.2, include = FALSE}
###are cp-knowers performing above chance for 4 and 5? 
cp.ms <- all.data.study2 %>%
  filter(Task == "SF", 
         CP_subset == "CP")%>%
  group_by(SID, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  ungroup()

cp.unit.t.test.4.2 <- t.test(subset(cp.ms, Task_item == 4)$mean, mu = .5, var.equal = TRUE)#nope
cp.unit.t.test.5.2 <- t.test(subset(cp.ms, Task_item == 5)$mean, mu = .5, var.equal = TRUE)#nope
```

Next, we tested whether there was evidence that CP-knowers, who have greater familiarity with the count list and its structure, were using this knowledge to succeed on the Unit Task. First, we replicated our previous finding that CP-knowers performed significantly better than subset knowers on the Unit Task ($\beta =$ `r round(cp.unit.2$coefficients[2,1], 2)`, $p =$ .002). As in Experiment 1, however, we again found that CP-knowers were at chance on the Unit Task for sets of 4 (`r round(cp.unit.t.test.4.2$statistic, 2)`, $p =$  `r round(cp.unit.t.test.4.2$p.value, 2)`) and 5 (`r round(cp.unit.t.test.5.2$statistic, 2)`, $p =$  `r round(cp.unit.t.test.5.2$p.value, 2)`).

Finally, we found that CP-knowers outperformed subset knowers on the Next Number  ($\beta =$ `r round(cp.nn.2$coefficients[2,1], 2)`, $p <$ .0001), and critically, that there was no difference in CP-knowers' Unit Task and Next Number performance ($\beta =$ `r round(cp.nn.sum$coefficients[2,1], 2)`, $p =$ .21), as shown in \ref{fig:exp_2_cp_sub}. Thus, we did find some evidence that CP-knowers who have access to the ordinal structure of the count list may be able to recruit this knowledge when reasoning about successor relations for these small numbers. Consistent with other work showing that the CP is not sufficient for Unit Task success, however, we found that CP-knowers were at chance for sets of 4 and 5.
```{r exp_2_cp_sub, fig.width=3.5, fig.height=1.9, fig.align = "center", fig.cap = "Mean Unit Task and Next Number performance for CP- and subset knowers in Exp. 2. Error bars represent 95\\% CIs computed by nonparametric bootstrap."}
all.data.study2 %>%
   filter(Task == "SF" | 
           Task == "Next_number")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Task = factor(Task, levels = c("Next_number", "SF"), 
                labels = c("Next Number", "Unit Task")), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP"), 
                            labels = c("Subset-knower", "CP-knower")))%>%
  group_by(CP_subset, Task, Task_item)%>%
  # summarise(n = n(), 
  #           mean = mean(Correct, na.rm = TRUE), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Task, group= Task)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size =7.5) + 
  theme(
        panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  facet_wrap(~CP_subset) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(0, 1)) + 
  scale_color_manual(values = task.pal)+ 
  labs(x = "Number queried", y = "Mean task performance")
```

# General Discussion
For several years before children are able to explicitly state that it is possible to add 1 to any number, they seem to have fragmented knowledge of the successor function, successfully implementing it for some familiar numbers, yet failing for others [@davidson2012]. How does this item-based knowledge arise, and what is its relationship to a more generalized understanding of this property of the natural numbers? We explored these questions in the current work in two ways. In Experiment 1 we asked whether demonstrating this item-based knowledge occurs only after CP acquisition, or whether subset knowers' previous failures could be explained by the fact that they are typically tested on numbers that fall outside their known number range. In Experiment 2, we explored the possible mechanisms underlying the origins of the successor function, testing whether such knowledge relied on knowledge of the count list, or could be explained by a set-mapping mechanism. 

In both Experiments, we found that subset knowers performed significantly better than chance on the Unit Task when tested on items that fell within their known number range. Further, we found evidence that subset knowers’ performance on the Unit Task tracked with their number word knowledge, and improved gradually as they accumulated more number word meanings. In Experiment 2, we found that subset knowers were not using the count list to succeed on the Unit Task; rather, it appears that these children were likely maintaining and updating these small sets using the Parallel Individuation system [@carey2004] and, if they had the appropriate number word for the set, to map it onto the final quantity. These findings are consistent with hypothesis that the Parallel Individuation system may form a foundational role in helping children acquire the number words \emph{one}, \emph{two}, and \emph{three} [@carey2004;@carey2019; see @sella2020 for similar findings]. These results further suggest that children are capable of using these memory-based representations to perform the set operations necessary to succeed in the Unit Task without the benefit of the count list's structure. Thus, these results suggest that children may begin to acquire item-based successor mappings quite early in numerical development, and that these mappings may be initially independent of count list knowledge.

Consistent with other work, however, we found that children’s successor knowledge increased after they acquired the CP, with CP-knowers out-performing subset knowers on the Unit Task in both Experiments. Further, we found some evidence that CP-knowers, who have some understanding of the count list and its relationship to cardinality, may actively recruit this knowledge in establishing these item-based successor mappings: In Experiment 2, we found that CP-knowers' Next Number performance was not significantly different than their Unit Task performance. While recent work has suggested that children eventually use the structure of the count list to make a full induction about the successor function [@cheung2017;@chu2020], our results indicate that even young CP-knowers may be starting to recognize the count list’s significance in solving these tasks. CP-knowers' performance on sets that fell outside the limits of working memory (4 and 5) and could only have been solved by using the count list indicates that this knowledge is not necessarily entailed by acquiring the CP, however. 

What role do these early item-based successor mappings play in leading children to the eventual conclusion that every number has a successor? One possibility is that these early item-based mappings may play an important role in establishing the basic set-operation mechanisms needed to perform the successor function. Our results suggest that this process may begin even before children learn the significance of the count routine, although it is constrained by children's number word knowledge. After children become CP-knowers, and their lexicon of known number words increases, the set of numbers for which they are able to reason about successor relations may increase, causing children to assert that it is possible to add +1 to some, but not all numbers. These localized networks may lay the ground for a larger induction about the successor function, as proposed by @davidson2012. This induction may be made when children learn that number words are recursively generated — and that the item-based successor function that they apply to some numbers actually extends to all numbers [@cheung2017;@chu2020;@schneider2020]. 

Taken together, our results suggest that children begin establishing the general conceptual framework which grounds successor function knowledge quite early in development, and is not dependent upon knowing the CP. Prior to learning the significance of the count routine and its relationship to cardinality, children use the numerical tools at their disposal, such as the Parallel Individuation system, to begin establishing the basic set-operations of the successor function. As children acquire more numerical tools and knowledge, they are incorporated into this limited successor knowledge, possibly forming the foundation for a larger induction about this logical principle. 
<!-- From intro: While much remains to be discovered about children's successor function acquisition, a potential framework for this process has begun to emerge. It is possible that children first begin learning about the successor function by establishing a network of localized successor relations, as proposed by @davidson2012. As children master more of the the count list, these new numbers may be gradually incorporated into this network, incrementally providing evidence that adding +1 to cardinalities results in a +1 change in the count list. Mastering the recursive structure of the count list may allow children to extend this item-based knowledge, supporting an induction about how the successor function is implicated in the recursive generation of number words [@schneider2020;@chu2020]. -->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
