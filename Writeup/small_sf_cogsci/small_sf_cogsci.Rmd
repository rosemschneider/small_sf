---
title: "Starting small: Exploring the origins of successor function knowledge"
bibliography: citations.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    <!-- \author{{\large \bf Rose M. Schneider},^1  {\large \bf Ashlie H. Pankonin},^2 {\large \bf Adena Schachner},^1, $\&$ {\large \bf David Barner}^1 -->
    <!-- \\ ^1Department of Psychology, University of California, San Diego \\ -->
    <!-- ^2 School of Speech, Language, and Hearing Sciences, San Diego State University} -->

abstract: >
    Although children in the US learn to accurately count sets by around 3.5 years of age, they often do not have a more general understanding of how the count list represents number. In particular, most children at this age fail to understand that adding 1 item to a set corresponds to counting up 1 word in the count list, and vice versa. Initially, children have piecemeal knowledge of this relation, and may know that adding 1 to "four" results in "five", but may lack generalized knowledge of how this relationship can be applied to any number. Currently, although we know that this generalized knowledge emerges at around 5.5 or 6 years of age in US children, we don't know when their earlier item-based learning begins, and therefore when they begin learning how the meanings of number words are related to one another -- a critical precursor to mathematical reasoning.
    
keywords: >
    Number; cognitive development; counting
    
output: cogsci2016::cogsci_paper
# final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, include = FALSE}
rm(list = ls())
require("knitr")
# opts_knit$set(root.dir = "~/Documents/Projects/small_sf/") #this is specific to RMS, change accordingly
library(tidyverse)
library(magrittr)
library(langcog)
library(lme4)
library(stringr)
library(RColorBrewer)
library(ggthemes)
library(memisc)
'%!in%' <- function(x,y)!('%in%'(x,y))
```

```{r}
####DATA LOADING AND MANAGEMENT
#load study 1 data
data.raw.study1 <- read.csv("../../Data/small_sf_study1.csv")
data.raw.study2 <- read.csv("../../Data/small_sf_study2.csv")

##Exclude any pilots and copypaste
data.raw.study1 %<>%
  filter(SID != "CopyPasteMe", 
         Exclusion_reason != "Pilot")%>%
  droplevels()%>%
  # dplyr::select(-X, -X.1, -X.2, -X.3, -X.4)%>%
  mutate(Age = as.numeric(as.character(Age)))

data.raw.study2 %<>%
  filter(SID != "CopyPasteMe", 
         Exclusion_reason != "Pilot")%>%
  droplevels()%>%
  # dplyr::select(-X, -X.1, -X.2, -X.3, -X.4)%>%
  mutate(Age = as.numeric(as.character(Age)))

#how many kids pre-exclusions?
#exp.1
pre.excl.exp.1 <- data.raw.study1 %>%
  distinct(SID, Age)%>%
  summarise(n = n())

#exp.2
pre.excl.exp2 <- data.raw.study2 %>%
  distinct(SID, Age)%>%
  summarise(n = n())

#Why are kids excluded?
#exp.1
excl.reason.exp1 <- data.raw.study1 %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude, Exclusion_reason)%>%
  group_by(Exclusion_reason)%>%
  summarise(n = n())
#exp.2
excl.reason.exp2 <- data.raw.study2 %>%
  filter(Exclude == 1)%>%
  distinct(SID, Exclude, Exclusion_reason)%>%
  group_by(Exclusion_reason)%>%
  summarise(n = n())

#### GLOBAL EXCLUSIONS
#exp.1
data.raw.study1 %<>%
  filter(Exclude != 1)%>%
  droplevels()
#exp.2
data.raw.study2 %<>%
  filter(Exclude != 1)%>%
  droplevels()

#### TRIAL/TASK EXCLUSIONS
#exp.1
data.raw.study1 %<>%
  mutate(Exclude_trial = ifelse(is.na(Exclude_trial), "0", as.character(Exclude_trial)))%>%
  filter(Exclude_trial != "1")

#exp.2
data.raw.study2 %<>%
  mutate(Exclude_trial = ifelse(is.na(Exclude_trial), "0", as.character(Exclude_trial)))%>%
  filter(Exclude_trial != "1")


#how many kids failed training for SF?
#exp.1
fail.sf.training.exp1 <- data.raw.study1 %>%
  filter(Task == "SF", 
         Trial_number == "Training", 
         Correct == "0")%>%
  summarise(n = n())

#exp.2
fail.sf.training.exp2 <- data.raw.study2 %>%
  filter(Task == "SF", 
         Trial_number == "Training", 
         Correct == "0")%>%
  summarise(n = n())

##exclude training for SF
#exp.1
data.raw.study1 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)))%>%
  filter(Trial_number != "Training")

##exclude training for SF
#exp.2
data.raw.study2 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)))%>%
  filter(Trial_number != "Training")

#Exclude training for NN
data.raw.study2 %<>%
  mutate(Trial_number = ifelse(is.na(Trial_number), "HC", as.character(Trial_number)), 
         Exclude_NN = ifelse((Task == "Next_number" & Trial_number == "1"), 1, 0))%>%
  filter(Exclude_NN != 1)

## RENAME ABOVE FOR SIMPLICITY
all.data.study1 <- data.raw.study1

all.data.study2 <- data.raw.study2
```

```{r, include = FALSE}
# Data validation
## Checking knower level classifications
#study 1
given.ms <- all.data.study1 %>%
  mutate(Knower_level = ifelse(Knower_level == "CP", 6, as.numeric(as.character(Knower_level)))) %>% #for function below
  filter(Task == "Give_N", 
         !is.na(Task_item))%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

#overall check - an N-knower should have gotten at least .67 mean performance for a given N
check <- given.ms %>%
  filter(Task_item == Knower_level)%>%
  filter(mean < .66)

if(length(check$SID) > 0) {
  print("CHECK STUDY 1 KLs")
}

#study 2
given.ms <- all.data.study2 %>%
  mutate(Knower_level = ifelse(Knower_level == "CP", 6, as.numeric(as.character(Knower_level)))) %>% #for function below
  filter(Task == "Give_N", 
         !is.na(Task_item))%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(as.numeric(as.character(Correct)), na.rm = TRUE))

#overall check - an N-knower should have gotten at least .67 mean performance for a given N
check <- given.ms %>%
  filter(Task_item == Knower_level)%>%
  filter(mean < .66)

if(length(check$SID) > 0) {
  print("CHECK STUDY 2 KLs")
}
```

```{r}
#change correct to numeric
all.data.study1 %<>%
  mutate(Correct = as.numeric(as.character(Correct)))

all.data.study2 %<>%
  mutate(Correct = as.numeric(as.character(Correct)))
```

```{r}
##add CP_subset
#by CP_subset
#Study 1 
all.data.study1 %<>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", 
                            "Subset"))
#Study 2 
all.data.study2 %<>%
  mutate(CP_subset = ifelse(Knower_level == "CP", "CP", 
                            "Subset"))
```

```{r highest_count}
##add highest count as a column
#study 1
hc.lookup <- all.data.study1 %>%
  filter(Task == "Highest_count")%>%
  dplyr::rename(highest_count = Response)%>%
  dplyr::select(SID, highest_count)

all.data.study1 <- right_join(all.data.study1, hc.lookup, by = "SID")

#HCs are being read in as factors - change to numeric
all.data.study1 %<>%
  mutate(highest_count = as.numeric(as.character(highest_count)))

#one kid didn't do highest count, so we will exclude them from analyses containing hc

#study 2
hc.lookup <- all.data.study2 %>%
  filter(Task == "Highest_count")%>%
  dplyr::rename(highest_count = Response)%>%
  dplyr::select(SID, highest_count)

all.data.study2 <- right_join(all.data.study2, hc.lookup, by = "SID")

#HCs are being read in as factors - change to numeric
all.data.study2 %<>%
  mutate(highest_count = as.numeric(as.character(highest_count)))

#one kid didn't do highest count, so we will exclude them from analyses containing hc
```

# Introduction
In the first few years of life, children accumulate knowledge of the components of symbolic number in isolation, gradually combining and integrating them to arrive at increasingly abstract numerical concepts. Throughout this process, however, their understanding of number is often strikingly limited, indicating fragile or absent mappings between components. A key example of this phenomenon is in children's protracted discovery that adding 1 to a set always denotes a number word 1 greater in the count list -- an understanding that reflects some implicit knowledge of the \emph{successor function}, a logical principle which states that the successor of any number \emph{n} is \emph{n}+1. For around 2 years after children have learned how to use the count routine to generate sets, they demonstrate piecemeal successor knowledge, and can implement it for only some numbers in their count list [@davidson2012; @cheung2017]. While there is growing evidence that children eventually generalize the successor function to \emph{all} numbers on the basis of their count list knowledge, there has been little work exploring their earlier item-based knowledge. Here, we investigate the origins of successor knowledge by exploring the mechanisms through which children acquire these localized mappings.

Children's generalization of the successor function comes several years after an extended process of acquiring the meanings of number words and the count routine. While most US children recite portions of the count list around 2 years of age [@fuson1988], it is not until around 2.5 years that they begin to understand the meanings of some number words [@wynn1990]. These "subset knowers" learn the first few number words sequentially and in isolation over ~18 months, but do not recognize the connection between the cardinalities represented by these number words and the count list. For example, a "two-knower" might be able to generate sets of \emph{one} and \emph{two}, while having no knowledge of \emph{three} [@wynn1992]. Around 4 years of age, however, children eventually make this connection, and realize how the last word said while counting indicates the cardinality of a set (i.e. the Cardinal Principle, or CP).

On some accounts, children acquire the CP by noticing an ismorphism between the cardinalities of their known number words and the ordinal structure of the count list [@carey2004; @sarnecka2008]; that is, children discover that the +1 difference in these cardinalities is reflected by a +1 difference in the count list, an insight that is compatible with some implicit knowledge of the successor function. This hypothesis has typically been evaluated using a paradigm known as the "Unit Task" [@sarnecka2008], in which an experimenter labels a set ("Look! Here are 4 frogs!") and hides it behind an occluder. The experimenter then adds 1 item and asks, "Are there 5 or 6 frogs now?". Compatible with the hypothesis that acquiring the CP entails implicit knowledge of the successor function, @sarnecka2008 found that CP-knowers were significantly above chance on this task for sets of 4 and 5, while subset knowers performed at chance. 

While CP-knowers in Sarnecka and Carey (2008) outperformed subset knowers, there is strong evidence that their Unit Task success did not indicate generalized successor knowledge, but instead reflected item-based knowledge of specific numbers; that is, many CP-knowers may only know that \emph{six} is 1 more than \emph{five}, rather than the more general principle that, for \emph{any} number, moving up in the count list corresponds to adding +1. First, while CP-knowers in Sarnecka and Carey (2008) performed above chance (50%), their average performance was far from ceiling (67%). Second, subsequent work has found that count list mastery is a stronger predictor of Unit Task performance than CP knowledge. For example, @davidson2012 found that CP-knowers who could count higher without error passed the Unit Task for more numbers in their count list, while less proficient counters were at chance for even well-known numbers. @cheung2017 subsequently replicated and extended this finding, showing that CP-knowers were not uniformly successful for larger numbers on the Unit Task until they could count quite high (>80) without error.  

While multiple studies have explored children's acquisition of generalized successor knowledge, there is little work on how they establish these initial item-specific mappings. One possibility is that, similar to Carey's (2004) proposal, these mappings are supported by understanding how changes in cardinality are reflected in the count list; that is, by reasoning about set operations through knowledge of number word relations. This account predicts that only CP-knowers should succeed on the Unit Task, as subset knowers have not yet recognized the connection between the count list and cardinality. @spaepen2018 recently found evidence for this hypothesis, showing that CP-knowers trained on a portion of the count list containing \emph{five}, \emph{six}, and \emph{seven} had greater Unit Task performance for these numbers in comparison to untrained children. Notably, subset knowers did not improve with counting training. Thus, it is possible that while the CP is not sufficient to impart generalized successor knowledge, it may be necessary for acquiring item-based mappings.

An alternative to this analogical mapping account, however, is that children may begin establishing item-based knowledge through mapping set operations directly to known number words, and without drawing upon any count list knowledge. For example, a child who knows that \emph{three} corresponds to a set of three items could pass the Unit Task through tracking the addition 1 to a set of 2, and then mapping the label \emph{three} directly to a nonsymbolic set representation. Critically, acquisition of the CP is not neccesary on this account, as children's item-based knowledge is drawn from mappings made directly from sets to known number words, rather than by reasoning about an isomorphism between the count list and cardinality. Thus, this account would predict that even subset knowers should have able to succeed on the Unit Task for known number words. 

One possible reason previous work concluded that Unit Task success was dependent on CP acquisition is that it tested numbers well outside subset knowers' known number range (e.g., 4, 5, and 6). Despite lower performance in comparison to CP-knowers on such "large" numbers, many subset knowers demonstrate above-chance Unit Task performance for sets of 1 and 2, numbers well within their known range [@sarnecka2008]. Additionally, there is some evidence that subset knowers can perform the basic set operations required to succeed on the Unit Task, even without having access to the count list's ordinal structure. @huttenlocher1994 found that children between 2.5 and 4 years were able to track the addition of 1 to sets of 1, 2, and 3, while @hughes1981 found that children as young as 3 years succeeded on a paradigm similar to the Unit Task for similarly small numbers. Together, these findings raise the possibility that children may begin learning these item-based mappings prior to acquiring the CP, and also that such mappings might initially rely on mappings made directly from sets to words. However, the majority of previous work using the Unit Task has not been able to test for this possibility because it has focused on numbers which are unknown to subset knowers. Further, much of this previous work did not explicitly test whether count list knowledge mediated item-based Unit Task performance. 

Here, we explore these two possible mechanisms of item-specific Unit Task performance through examining subset and CP-knowers' Unit Task performance for very small sets (1 - 5). In Experiment 1, we find that Unit Task success is not dependent on the CP, but rather familiarity with specific numbers, and also that it is unrelated to general count list knowledge. In Experiment 2 we further explored the role of count list knowledge by comparing children's performance on the Unit Task to the Next Number task, a stronger measure of count list knowledge, and again find no evidence that subset knowers are drawing upon the count list to form these item-based mappings. 

# Experiment 1
## Method
The methods and analyses were pre-registered (https://tinyurl.com/tabob4l). All methodological and analytical choices were as pre-registered, unless stated otherwise in-text.

### Participants
```{r exp1.demos}
exp.1.demos.age <- all.data.study1 %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = round(mean(Age, na.rm = TRUE), 2), 
            sd_age = round(sd(Age, na.rm = TRUE), 2))%>%
  mutate(total.n = sum(n))

exp1.demos.sex <- all.data.study1 %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))


### by n-knower level
n.levels.exp1 <- all.data.study1 %>%
  group_by(Knower_level)%>%
  distinct(SID, Age, Knower_level)%>%
  summarise(n = n(),
            mean_age = round(mean(Age, na.rm = TRUE), 2),
            sd_age = round(sd(Age, na.rm = TRUE),2))

```
We recruited `r exp.1.demos.age$total.n[1]` participants between the ages of 2 and 4 years (`r exp1.demos.sex$n[1]` female, $M_{age} =$ `r round(mean(all.data.study1$Age, na.rm = TRUE), 2)`, $SD_{age} =$ `r round(sd(all.data.study1$Age, na.rm = TRUE),2)`, range $=$ `r round(min(all.data.study1$Age),2)`-`r round(max(all.data.study1$Age),2)` years) from local preschools and the surrounding community in San Diego, US. Fifty-two of these children were classified as subset knowers and `r subset(exp.1.demos.age, CP_subset == "CP")$n` were classified as CP-knowers using the Give-N task. The breakdown of \emph{N}-knower level classifications is shown in Table \ref{tab:demos1}.

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\hline
\textbf{Knower-level}  & \textbf{1}   & \textbf{2}  & \textbf{3} & \textbf{4}  & \textbf{CP}   \\
\hline
\emph{n}               & `r n.levels.exp1$n[1]` & `r n.levels.exp1$n[2]` & `r n.levels.exp1$n[3]`  & `r n.levels.exp1$n[4]`  & `r n.levels.exp1$n[5]` \\
$M_{age}$  & `r n.levels.exp1$mean_age[1]` & `r n.levels.exp1$mean_age[2]` & `r n.levels.exp1$mean_age[3]`  & `r n.levels.exp1$mean_age[4]`  & `r n.levels.exp1$mean_age[5]`  \\
$SD_{age}$ & `r n.levels.exp1$sd_age[1]` &  `r n.levels.exp1$sd_age[2]` & `r n.levels.exp1$sd_age[3]` & `r n.levels.exp1$sd_age[4]` & `r n.levels.exp1$sd_age[5]` \\
\hline
\end{tabular} 
\caption{Demographics for all participants by knower level classification.}
\label{tab:demos1}
\end{table}

<!-- \begin{table}[h] -->
<!-- \centering -->
<!-- \begin{tabular}{c c c }  -->
<!--  \hline -->
<!--  \emph{N}-knower level & \emph{n} & $M_{age}$ (SD) \\ -->
<!--  \hline -->
<!--  1-knower & `r n.levels.exp1$n[1]` & `r n.levels.exp1$mean_age[1]` (`r n.levels.exp1$sd_age[1]`)\\ -->
<!--  2-knower & `r n.levels.exp1$n[2]` & `r n.levels.exp1$mean_age[2]` (`r n.levels.exp1$sd_age[2]`)\\  -->
<!--  3-knower & `r n.levels.exp1$n[3]` & `r n.levels.exp1$mean_age[3]` (`r n.levels.exp1$sd_age[3]`)\\ -->
<!--  4-knower & `r n.levels.exp1$n[4]` & `r n.levels.exp1$mean_age[4]` (`r n.levels.exp1$sd_age[4]`)\\ -->
<!--  CP-knower & `r n.levels.exp1$n[5]` & `r n.levels.exp1$mean_age[5]` (`r n.levels.exp1$sd_age[5]`)\\ -->
<!--  \hline -->
<!-- \end{tabular} -->
<!-- \caption{Demographics for all participants by knower level classification.} -->
<!-- \label{tab:demos1} -->
<!-- \end{table}  -->

### Procedure
Children were tested individually in a quiet spot within the classroom or museum or in a room set apart from the classroom. Participants received the tasks in a fixed order (Unit Task, Give-N, and Highest Count).

#### Unit Task 
We assessed item-based successor knowledge with the Unit Task [@sarnecka2008]. The experimenter presented children with an opaque container and some small fish, saying, "This is my fish bowl and these are my fish!" The experimenter then placed between 1 and 5 fish in the bowl, briefly showed them to the child, and said, "Look, \emph{N} fish are swimming in the fish bowl." The experimenter then placed a lid over the container and asked, "How many fish are in the fish bowl?" Children were given two opportunities to pass this memory check; if they failed both, the experimenter told them how many fish there were, and proceeded with the remainder of the trial. After the memory check, the experimenter added 1 fish to the container before asking, “Are there \emph{N}+1 or \emph{N}+2 fish now?”. Order of alternatives was counterbalanced across trials.

Participants completed a training trial with feedback in which only 1 fish was added to an empty bowl. Once children passed this training trial, they received 10 test trials with neutral feedback. A correct response for a given \emph{N} was \emph{N}+1. “I don’t know” responses were coded as incorrect. Only numeric responses were included in analyses; non-numeric answers were excluded. Trials were classified as either within or outside of a child’s \emph{N}-knower level.

#### Give-N
We used a titrated version of Give-N to assess children's knower level. The experimenter provided children with 10 plastic objects (e.g., bananas) and a small plate. After familiarizing the child with the game, the experimenter asked them to put \emph{N} items on the plate After the child finished, the experimenter asked “Is that \emph{N}? Can you count to make sure?” If the child recognized an error, they were permitted to fix the set. If the child succeeded on a requested \emph{N}, the experimenter requested \emph{N}+1 on the next trial, up to 6 items. If the child failed on a given \emph{N}, the experimenter requested \emph{N}-1 on the next trial. This pattern continued until the child’s knower level could be identified. Children were defined as \emph{N}-knowers if they correctly provided \emph{N} on at least two out of the three trials where \emph{N} was requested, and did not give that \emph{N} more than once for another number. Children who correctly generated sets of 6 at least 2 out of 3 times when requested were classified as CP-knowers. 

#### Highest Count. 
We used the Highest Count task as a measure of counting experience beyond the CP stage, as well as to test whether, even in the subset stages, knowledge of counting procedures might enhance knowledge of relations between number words. An experimenter introduced the task to the child by saying, “In this game, I want you to count as high as you can! Can you start counting for me with \emph{one}?" A child’s Highest Count was the largest number counted to before making an error, or the point at which the child did not know how to continue.

```{r model_setup_exp1, include = FALSE}
model.df.study1 <- all.data.study1 %>%
  filter(Task == "SF", 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)))
```

```{r ss_exp_1_chance, include = FALSE}
##DO SS KNOWERS PERFORM SIGNIFICANTLY ABOVE CHANCE
#filter down to subset knowers

subset.chance.model <- glmer(Correct ~ 1 + (1|SID), family = "binomial", 
                               data = subset(model.df.study1, CP_subset == "Subset"))
subset.chance.overall <- summary(subset.chance.model)

```

```{r ss_exp_1_kl, include = FALSE}
##DO SS KNOWERS PERFORM ABOVE CHANCE WITHIN KL
##add to model.df whether a trial was within or outside knower level 
model.df.study1.within <- model.df.study1 %>%
  filter(CP_subset != "CP")%>%
  mutate(within_kl = ifelse(Task_item <= as.numeric(as.character(Knower_level)), "Within", "Outside"))

subset.chance.within.kl <- glmer(Correct ~ 1 + (1|SID), family = "binomial", 
                                 data = subset(model.df.study1.within, within_kl == "Within"))

subset.chance.within.kl.sum <- summary(subset.chance.within.kl)

```

```{r ss_exp_1_hc, include = FALSE}
##DO SS KNOWERS WITH HIGHER HC PERFORM BETTER
##get highest count for each kid, add to model.df 
subset.hc <- glmer(Correct ~ highest_count + (1|SID), 
                   family = "binomial", data = subset(model.df.study1, CP_subset == "Subset"))

#compare to base
hc.ss.anova <- anova(subset.chance.model, subset.hc, test = 'LRT')

##WHAT ABOUT WITHIN THEIR KL
subset.hc.within <- glmer(Correct ~ highest_count + (1|SID), 
                          family = "binomial", data = subset(model.df.study1.within, within_kl == "Within"))

#lrt to compare
anova(subset.chance.within.kl, subset.hc.within, test = 'LRT')

##DO CP KNOWERS WITH HIGHER HC PERFORM BETTER

```

## Results and Discussion
```{r means_1, include=FALSE}

means_within <- model.df.study1.within %>%
  filter(Task == "SF")%>%
  group_by(CP_subset, within_kl)%>%
  summarise(mean_correct = mean(Correct, na.rm = TRUE), 
            sd_correct = sd(Correct, na.rm = TRUE))

means_overall <- all.data.study1 %>%
  filter(Task == "SF")%>%
  group_by(CP_subset)%>%
  summarise(mean_correct = mean(Correct, na.rm = TRUE), 
            sd_correct = sd(Correct, na.rm = TRUE))
```

```{r exp_1_cp_subset, fig.pos = "tb", fig.width= 2.4, fig.height=1.8, fig.align = "center", fig.cap = "Mean Unit Task performance for CP- and subset knowers in Exp. 1. Error bars represent 95\\% CIs computed by nonparametric bootstrap."}

cp.subset.pal <- c("#CAB2D6", "#6A3D9A" )
all.data.study1 %>%
    filter(Task == "SF")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP"), 
                            labels = c("Subset-knower", "CP-knower")))%>%
  group_by(Task_item, CP_subset)%>%
   # summarise(mean = mean(Correct, na.rm = TRUE), 
   #          n = n(), 
   #          sd = sd(Correct, na.rm = TRUE), 
   #          se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = CP_subset, group= CP_subset)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey", size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size = 9) + 
  theme(panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(.25, 1)) + 
  scale_color_manual(values = cp.subset.pal) +
  labs(x = "Starting set size", y = "Mean Unit performance", 
       color = "Knower level")
```

```{r exp_1_t_tests, include = FALSE}
subset.ms <- all.data.study1 %>%
  filter(Task == "SF", 
         CP_subset == "Subset")%>%
  group_by(SID, Knower_level, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))

##1-knowers
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "1" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS

##2-knowers
two_knower.t.1 <- t.test(subset(subset.ms, Knower_level == "2" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##Significant, t(17) = 6.65, p < .0001
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "2" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS

#3-knowers
three_t.test.1 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 1)$mean, mu = .5, var.equal = TRUE) ##Significant, t(13) = 8.83, p < .0001
three_t.test.2 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 2)$mean, mu = .5, var.equal = TRUE) ##Significant, t(13) = 3.80, p = .002
three_t.test.3 <- t.test(subset(subset.ms, Knower_level == "3" & Task_item == 3)$mean, mu = .5, var.equal = TRUE) ##Marginal, t(13) = 2.12, p = .05
t.test(subset(subset.ms, Knower_level == "3" & Task_item == 4)$mean, mu = .5, var.equal = TRUE) ##NS
t.test(subset(subset.ms, Knower_level == "3" & Task_item == 5)$mean, mu = .5, var.equal = TRUE) ##NS
```

```{r cp_subset, include = FALSE}
###TESTING FOR DIFFERENCE BETWEEN CP AND SUBSET KNOWERS
model.cp.study1 <- all.data.study1 %>%
  filter(Task == "SF", 
         !is.na(highest_count), 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)), 
          age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)))

#construct base model with age
cp.subset.base <- glmer(Correct ~ age.c + (1|SID), 
                        family = "binomial", data = model.cp.study1)

#add knower level - does this explain additional variance?
cp.subset.kl <- glmer(Correct ~ CP_subset + age.c + (1|SID), 
                      family = "binomial", data = model.cp.study1)

#compare - does KL improve the fit of the base model? 
anova(cp.subset.base, cp.subset.kl, test = 'LRT')
cp.subset.kl.sum <- summary(cp.subset.kl)
```

```{r cp_chance, include = FALSE}
###are cp-knowers performing above chance for 4 and 5? 
cp.ms <- all.data.study1 %>%
  filter(Task == "SF", 
         CP_subset == "CP")%>%
  group_by(SID, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  ungroup()

cp.unit.t.test.4 <- t.test(subset(cp.ms, Task_item == 4)$mean, mu = .5, var.equal = TRUE)#nope
cp.unit.t.test.5 <- t.test(subset(cp.ms, Task_item == 5)$mean, mu = .5, var.equal = TRUE)#nope
```

```{r cp_hc, include = FALSE}
##ARE CP KNOWERS WITH HIGHER HC FOR ACCURATE
model.cp.study1 %<>%
  filter(!is.na(highest_count))

cp.hc.base <- glmer(Correct ~ age.c + (1|SID), 
               family = "binomial", data = subset(model.cp.study1, CP_subset == "CP"))
cp.hc <- glmer(Correct ~ highest_count.c+ age.c + (1|SID), 
               family = "binomial", data = subset(model.cp.study1, CP_subset == "CP"))

cp.hc.anova <- anova(cp.hc.base, cp.hc, test ='LRT')

cp.hc.summary <- summary(cp.hc)
```

In our first analysis we tested whether, consistent with previous work [@sarnecka2008; @spaepen2018], CP-knowers were more accurate than subset knowers on the Unit Task. To test this, we built a generalized linear mixed effects model (GLMEM)\footnote{All mixed models were fit in \texttt{R} using the \texttt{lme4} package.} predicting Unit Task performance from CP-knower status and age with a random effect of participant. Compatible with prior work, this model indicated that subset knowers had significantly less accurate Unit Task performance in comparison to CP-knowers (`r round(cp.subset.kl.sum$coefficients[2], 2)`, $p =$ .02;  Figure \ref{fig:exp_1_cp_subset}). Despite this greater overall accuracy, we also found evidence of item-based success in this group, and that acquiring the CP does not guarantee implicit successor knowledge [@davidson2012; @cheung2017; @spaepen2018], with CP-knowers performing at chance for sets of 4 (`r round(cp.unit.t.test.4$statistic, 2)`, $p =$  `r round(cp.unit.t.test.4$p.value, 2)`) and 5 (`r round(cp.unit.t.test.5$statistic, 2)`, $p =$  `r round(cp.unit.t.test.5$p.value, 2)`).

While subset knowers had significantly lower mean Unit Task accuracy (\emph{M} = `r round(means_overall$mean_correct[2], 2)`) in comparison to CP-knowers (\emph{M} = `r round(means_overall$mean_correct[1], 2)`), a null GLMEM predicting Unit Task performance with a random effect of subject indicated that their overall performance was significantly different from chance (Wald \emph{Z} $=$ `r round(subset.chance.overall$coefficients[3], 2)`, $p =$ .0007), as shown in Figure \ref{fig:exp_1_cp_subset}. Looking specifically at numbers within their known range, we again found that subset knowers performed significantly better than chance (Wald \emph{Z} $=$ `r round(subset.chance.within.kl.sum$coefficients[3], 2)`, $p$ < .0001), with higher accuracy for items which were within their known range (\emph{M} = `r round(means_within$mean_correct[2], 2)`) as opposed to unknown numbers (\emph{M} = `r round(means_within$mean_correct[1], 2)`). Thus, although subset knowers were less accurate than CP-knowers on this task, their performance was above chance overall. Additionally, we find evidence that this effect is driven by their knowledge of known number words.

The effects of known vs. unknown number trials for subset knowers is shown in Figure \ref{fig:exp_1_unit}. Follow-up analyses indicated that subset knowers' performance Unit Task performance roughly tracked with their Knower level, such that 2-knowers were above chance for sets of 1 (\emph{t}(17) = `r round(two_knower.t.1$statistic, 2)`, $p$ < .0001), and 3-knowers were above chance for sets of 1 (\emph{t}(13) = `r round(three_t.test.1$statistic, 2)`, $p$ < .0001), 2 (\emph{t}(13) = `r round(three_t.test.2$statistic, 2)`, $p$ $=$ .002), and marginally for sets of 3 (\emph{t}(13) = `r round(three_t.test.3$statistic, 2)`, $p =$ .05). One-knowers were not significantly above chance for any set size (all \emph{p}s > .05). While these analyses are under-powered, nevertheless they suggest that subset knowers' Unit Task performance tracks roughly with their Knower-level.

Finally, we tested whether children's Unit Task performance was related to their knowledge of the count list, which we assessed through measuring children's highest errorless count. We built another generalized linear mixed effects model predicting Unit Task performance from highest count (centered and scaled) and age, with a random effect of participant. Because our previous analyses revealed that Unit Task performance differed significantly by Knower-level, we conducted this analysis separately for subset and CP-knower knowers. Likelihood Ratio Tests indicated that the addition of the highest count term did not significantly improve the fit of the model for overall performance either for subset knowers ($\chi^2$ $=$ `r round(hc.ss.anova$Chisq[2],2)`, $p$ $=$ 0.45), or for CP-knowers ($\chi^2$ $=$ `r round(cp.hc.anova$Chisq[2],2)`, $p$ $=$ 0.14). Thus, these analyses suggest that children's early item-based Unit Task success may not be related to their knowledge of the count list, at least at the level we could detect with the Highest Count measure.

Together, these results indicate that, despite demonstrating lower accuracy in comparison to CP-knowers on the Unit Task, subset knowers can map the results of the set operations in this task to a range of small number words (1-5) at above-chance levels, and that this success is driven by items within their Knower level. Critically, however, we found that performance was not related to counting proficiency for either subset knowers or, surprisingly, CP knowers, suggesting that some children may not succeed in this task through reasoning about relations between number words. This result is compatible with the hypothesis that the origins of children's implicit successor knowledge may not lie in understanding the structure of the count list, but potentially through mapping the results of set operations directly to known number words. 

```{r exp_1_unit, fig.pos = "tb", fig.width=3.5, fig.height=1.6, fig.align = "center", fig.cap = "Mean Unit Task performance for subset knowers in Exp. 1, grouped by knower level. Error bars represent 95\\% CIs computed by nonparametric bootstrap. Three- and 4-knowers shown together."}
#visualization 
all.data.study1 %>%
  filter(Knower_level == "1" | 
           Knower_level == "2" | 
           Knower_level == "3" | 
           Knower_level == "4" )%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Knower_level.combined = ifelse((Knower_level == "4" | Knower_level == "3"), "3- & 4-knowers", as.character(Knower_level)),
         Knower_level.combined = factor(Knower_level.combined, levels= c("1", "2", "3- & 4-knowers", "CP"), 
                               labels = c("1-knowers", "2-knowers", "3- & 4-knowers", "CP-knowers")))%>%
  filter(Task == "SF")%>%
  group_by(Task_item, Knower_level.combined)%>%
  # summarise(mean = mean(Correct, na.rm = TRUE),
  #           n = n(), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Knower_level.combined, group= Knower_level.combined)) +
  geom_point(size = 1, 
             show.legend = FALSE) + 
  geom_line(size = .5, 
            show.legend = FALSE) +
  geom_hline(yintercept = .5, linetype = "dashed", color = "grey", size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5, 
                show.legend = FALSE) +
  theme_bw(base_size = 9) + 
  theme(legend.position = "top", 
        panel.grid = element_blank()) + 
  facet_wrap(~Knower_level.combined, ncol = 3) +
  langcog::scale_color_solarized("Knower level") +
  labs(x = "Starting set size", y = "Mean Unit performance")
```

One limitation in Experiment 1, however, was that our measure of count list knowledge (Highest Count) varies substantially across children, and does not systematically test children's knowledge of relations between number words, but may reflect a rote-memorized list. Thus, in Experiment 2, we included the Next Number task to assess their knowledge of these number word relations.

# Experiment 2
To test whether children in Experiment 1 were succeeding through deploying item-based mappings between known number words and sets stored in working memory, or through an analogical mapping based on a portion of the count list containing these known numbers, we compared subset and CP-knowers' Unit Task performance with their Next Number performance. This task requires children to count up from an arbitrary point in the count list in response to a prompt (e.g., "\emph{N}, what comes next?"). The logic of adding this task was that, if children are using knowledge of relations between number words to succeed on the Next Number task, then we should find that children who are able to pass the Unit Task for a given \emph{N} should also be able to name the number after \emph{N}. If, however, children are solving the Unit Task by mapping directly from representations of sets to known number words, then we should find no relationship between these two tasks. 

## Method
The methods and analyses were pre-registered (https://tinyurl.com/uh5wyfa).

### Participants
```{r exp2.demos}
exp.2.demos.age <- all.data.study2 %>%
  distinct(SID, CP_subset, Age)%>%
  group_by(CP_subset)%>%
  summarise(n = n(), 
            mean_age = round(mean(Age, na.rm = TRUE), 2), 
            sd_age = round(sd(Age, na.rm = TRUE), 2))%>%
  mutate(total.n = sum(n))

exp2.demos.sex <- all.data.study2 %>%
  distinct(SID, Sex)%>%
  group_by(Sex)%>%
  summarise(n = n())%>%
  mutate(total.n = sum(n))


### by n-knower level
n.levels.exp2 <- all.data.study2 %>%
  group_by(Knower_level)%>%
  distinct(SID, Age, Knower_level)%>%
  summarise(n = n(),
            mean_age = round(mean(Age, na.rm = TRUE), 2),
            sd_age = round(sd(Age, na.rm = TRUE),2))

```

We recruited `r exp.2.demos.age$total.n[1]` participants between 2 and 4 years of age (`r subset(exp2.demos.sex, Sex == "F")$n[1]` female, $M_{age} =$ `r round(mean(all.data.study2$Age, na.rm = TRUE), 2)`, $SD_{age} =$ `r round(sd(all.data.study2$Age, na.rm = TRUE),2)`, range $=$ `r round(min(all.data.study2$Age),2)`-`r round(max(all.data.study2$Age),2)` years) from local preschools and the surrounding community in San Diego, US. Forty-five of these children were classified as subset knowers and `r subset(exp.2.demos.age, CP_subset == "CP")$n` were classified as CP-knowers. The breakdown of \emph{N}-knower level classifications is shown in Table \ref{tab:demos2}.

\begin{table}[h]
\centering
\begin{tabular}{ccccccc}
\hline
\textbf{Knower-level}  & \textbf{1}   & \textbf{2}  & \textbf{3} & \textbf{4}  & \textbf{5} & \textbf{CP}   \\
\hline
\emph{n}               & `r n.levels.exp2$n[1]` & `r n.levels.exp2$n[2]` & `r n.levels.exp2$n[3]`  & `r n.levels.exp2$n[4]`  & `r n.levels.exp2$n[5]` & `r n.levels.exp2$n[6]` \\
$M_{age}$  & `r n.levels.exp2$mean_age[1]` & `r n.levels.exp2$mean_age[2]` & `r n.levels.exp2$mean_age[3]`  & `r n.levels.exp2$mean_age[4]`  & `r n.levels.exp2$mean_age[5]` & `r n.levels.exp2$mean_age[6]`\\
$SD_{age}$ & `r n.levels.exp2$sd_age[1]` &  `r n.levels.exp2$sd_age[2]` & `r n.levels.exp2$sd_age[3]` & `r n.levels.exp2$sd_age[4]` & -- & `r n.levels.exp2$sd_age[6]`\\
\hline
\end{tabular} 
\caption{Demographics for all participants by knower level classification.}
\label{tab:demos2}
\end{table}

### Procedure
Stimuli and methods were identical to Experiment 1 with two exceptions. First, we added the Next Number task to test children's knowledge of relations between the number words queried in the Unit Task (1-5). Children were given a prompt, and then asked what number came next. Children received a training trial with 'one', during which they received feedback, and then received 10 additional trials. Items were present in a pseudo-randomized order, and each number was queried twice. As in Experiment 1, participants received the tasks in a fixed order (Unit Task, Next Number, Give-N, and Highest Count). 

Second, we also controlled for the possibility that children could succeed on the Unit Task through subitizing the final set when the lid was removed to add fish during each trial. In Experiment 2, we used a lid with a small slot through which the experimenter could insert this fish, thus preventing children from observing the final set. 

## Results and Discussion
```{r exp_2_model, include= FALSE}
##This is for Next number AND SF analyses
model.df.study2 <- all.data.study2 %>%
  filter(Task == "SF" | 
           Task == "Next_number", 
         !is.na(Correct))%>%
  mutate(highest_count = as.numeric(as.character(highest_count)), 
         age.c = as.vector(scale(Age, center = TRUE, scale=TRUE)), 
         highest_count.c = as.vector(scale(highest_count, center = TRUE, scale = TRUE)))
```

```{r nn_vs_unit, include = FALSE}
###COMPARING NN VS UNIT FOR SS OVERALL

#make base
nn.v.sf.base <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset"))
#add task
nn.v.sf.task <- glmer(Correct ~ Task + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset"))
#compare
anova(nn.v.sf.base, nn.v.sf.task, test= 'LRT')#significant
subset.nn.unit <- summary(nn.v.sf.task)
```

```{r ss-cp, include = FALSE}
##DOES HC PREDICT UNIT FOR SUBSET and CP
ss.hc.base.2 <- glmer(Correct ~ age.c + (1|SID), family = "binomial", 
                    data = subset(model.df.study2, Task == "SF" &CP_subset == "Subset"))
ss.hc.2 <- glmer(Correct ~ highest_count.c + age.c + (1|SID), family = "binomial", 
                    data = subset(model.df.study2, Task == "SF" &CP_subset == "Subset"))
ss.hc.anova.2 <- anova(ss.hc.base.2, ss.hc.2, test = 'lrt')


##now cp knowers
model.df.hc <- model.df.study2 %>%
  filter(!is.na(highest_count))

cp.hc.base.2 <- glmer(Correct ~ age.c + (1|SID), family = "binomial", 
                 data = subset(model.df.hc, Task == "SF" & CP_subset == "CP"))
cp.hc.2 <- glmer(Correct ~ highest_count.c + age.c + (1|SID), family = "binomial", 
                  data = subset(model.df.hc, Task == "SF" & CP_subset == "CP"))
cp.hc.anova.2 <- anova(cp.hc.base.2, cp.hc.2, test = 'lrt')
```

```{r nn_unit_within, include = FALSE}
###TESTING WHETHER NN AND UNIT DIFFER FOR ITEMS WITHIN KL
#add a term indicating whether item is within/outside knower level
model.df.study2.within <- model.df.study2 %>%
  mutate(within_kl = ifelse(Task_item <= as.numeric(as.character(Knower_level)), "Within", "Outside"), 
         within_kl = factor(within_kl))

#make base
within.nn.v.sf.base <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & 
                                                         within_kl == "Within"))
#add task
within.nn.v.sf.task <- glmer(Correct ~ Task + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & 
                                                         within_kl == "Within"))
#compare
anova(within.nn.v.sf.base, within.nn.v.sf.task, test= 'LRT') # significant
nn.unit.subset.within <- summary(within.nn.v.sf.task)

```

```{r ss_unit_exp2, include = FALSE}
##SS UNIT CHANCE TEST
subset.sf.chance <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2, CP_subset == "Subset" & Task == "SF"))

subset.unit.exp2 <- summary(subset.sf.chance)
```

```{r ss_unit_exp2_within, include = FALSE}
##SS UNIT WITHIN CHANCE TEST
subset.sf.chance.within <- glmer(Correct ~ 1 + (1|SID), 
                      family = 'binomial', data = subset(model.df.study2.within, CP_subset == "Subset" & Task == "SF" 
                                                         & within_kl == "Within"))
subset.unit.known.exp2 <- summary(subset.sf.chance.within)
```

```{r, cp_nn_compare, include = FALSE}
####UNIT NN COMPARISON####
model.cp.study2 <- model.df.study2 %>%
  filter(!is.na(highest_count))
cp.nn.compare <- glmer(Correct ~ Task + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, CP_subset == "CP"))
cp.nn.sum <- summary(cp.nn.compare)

##UNIT##
cp.unit.base <- glmer(Correct ~ age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF"))
#now add CP-subset for comparison
cp.unit.compare <- glmer(Correct ~ factor(CP_subset, levels = c("Subset", "CP")) + age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF"))
#compare
anova(cp.unit.base, cp.unit.compare, test= 'LRT')#significant
cp.unit.2 <- summary(cp.unit.compare)

#highest count with unit
cp.unit.hc.base <- glmer(Correct ~ age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF" & CP_subset == "CP"))
cp.unit.hc. <- glmer(Correct ~ highest_count.c + age.c + (1|SID), 
                       family = "binomial", data = subset(model.cp.study2, Task == "SF" & CP_subset == "CP"))
#compare
anova(cp.unit.hc.base, cp.unit.hc., test = 'LRT')

####Next Number####
#make base
cp.subset.base.nn <- glmer(Correct ~ age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))


#add cp_subset
cp.subset.kl.nn <- glmer(Correct ~ factor(CP_subset, levels = c("Subset", "CP")) + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))
#compare
anova(cp.subset.base.nn, cp.subset.kl.nn, test = 'LRT')
cp.nn.2 <- summary(cp.subset.kl.nn)

#add highest_count
cp.subset.hc.nn <- glmer(Correct ~ highest_count.c + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number")) ##NB model is failing to converge, need to check if this is a big deal
with(cp.subset.hc.nn@optinfo$derivs,max(abs(solve(Hessian,gradient)))<2e-3) #we're okay
#compare
anova(cp.subset.base.nn, cp.subset.hc.nn, test = 'LRT') #hc significantly adds to base model
summary(cp.subset.hc.nn)

##Add KL to HC model
nn.plus.kl <-  glmer(Correct ~ CP_subset + highest_count.c + age.c + (1|SID), 
                      family = 'binomial', data = subset(model.cp.study2, Task == "Next_number"))
#compare
anova(cp.subset.hc.nn, nn.plus.kl, test = 'LRT') #cp and NN
summary(nn.plus.kl)
```

```{r cp_chance.2, include = FALSE}
###are cp-knowers performing above chance for 4 and 5? 
cp.ms <- all.data.study2 %>%
  filter(Task == "SF", 
         CP_subset == "CP")%>%
  group_by(SID, Task_item)%>%
  summarise(mean = mean(Correct, na.rm = TRUE))%>%
  ungroup()

cp.unit.t.test.4.2 <- t.test(subset(cp.ms, Task_item == 4)$mean, mu = .5, var.equal = TRUE)#nope
cp.unit.t.test.5.2 <- t.test(subset(cp.ms, Task_item == 5)$mean, mu = .5, var.equal = TRUE)#nope
```

```{r means, include = FALSE}
means_overall_2 <- all.data.study2 %>%
  filter(Task == "SF")%>%
  group_by(CP_subset)%>%
  summarise(mean_correct = mean(Correct, na.rm = TRUE))

means_within_2 <- model.df.study2.within %>%
  filter(Task == "SF", 
         CP_subset == "Subset")%>%
  group_by(within_kl)%>%
  summarise(mean_correct = mean(Correct, na.rm = TRUE))

###means nn
means_overall_2.nn <- all.data.study2 %>%
  filter(Task == "Next_number")%>%
  group_by(CP_subset)%>%
  summarise(mean_correct = mean(Correct, na.rm = TRUE))

means_within_2.nn <- model.df.study2.within %>%
  filter(Task == "Next_number", 
         CP_subset == "Subset")%>%
  group_by(within_kl)%>%
  summarise(mean_correct = mean(Correct, na.rm = TRUE))
```

Once again, we found that CP-knowers performed significantly better than subset knowers on the Unit Task ($\beta =$ `r round(cp.unit.2$coefficients[2,1], 2)`, $p =$ .002; \emph{M} = `r round(means_overall_2$mean_correct[1], 2)`). As in Experiment 1, however, CP-knowers were again at chance on the Unit Task for sets of 4 (`r round(cp.unit.t.test.4.2$statistic, 2)`, $p =$  `r round(cp.unit.t.test.4.2$p.value, 2)`) and 5 (`r round(cp.unit.t.test.5.2$statistic, 2)`, $p =$  `r round(cp.unit.t.test.5.2$p.value, 2)`). Additionally, we replicated the finding from Experiment 1 that, despite having lower accuracy in comparison to CP-knowers, subset knowers were above chance on the Unit Task overall (Wald \emph{Z} $=$ `r round(subset.unit.exp2$coefficients[3], 2)`, $p =$ .012; \emph{M} = `r round(means_overall_2$mean_correct[2], 2)`), and that this effect was driven by numbers within their known number range (Wald \emph{Z} $=$ `r round(subset.unit.exp2$coefficients[3], 2)`, $p =$ .0002; \emph{M} = `r round(means_within_2$mean_correct[2], 2)`). As in Experiment 1, we again found that Highest Count was not significantly predictive of Unit Task performance for either subset ($\chi^2$ = `r round(ss.hc.anova.2$Chisq[2], 2)`, $p$ = 0.28) or CP-knowers ($\chi^2$ = `r round(cp.hc.anova.2$Chisq[2], 2)`, $p$ = 0.27)

While we did not find a relationship between children's Highest Count and their Unit Task performance, in our next set of analyses we explored their Next Number performance to investigate whether there was evidence that children were using knowledge of the count list to succeed on the Unit Task. We tested this separately in subset and CP-knowers by comparing performance on the both tasks with a GLMEM predicting task performance (Unit or Next Number) with a random effect of participant.\footnote{Model specification: \texttt{Correct $\sim$ Task + ( 1 | subject)}} Interestingly, this model indicated that subset knowers' Next Number performance was significantly lower than their Unit Task performance, both overall ($\beta =$ -`r round(subset.nn.unit$coefficients[2,1], 2)`, $p$ < .0001; \emph{M} = `r round(means_overall_2.nn$mean_correct[2],2)`), and even for numbers within their known number range ($\beta =$ `r round(nn.unit.subset.within$coefficients[2,1], 2)`, $p$ < .0001; \emph{M} = `r round(means_within_2.nn$mean_correct[2], 2)`), as shown in Figure \ref{fig:exp_2_unit_nn}. This difference in performance between the Unit and Next Number tasks suggest that subset knowers are not drawing upon their knowledge of the count list to reason about successor relations for small numbers, but may instead be mappings between small set representations stored in working memory and known number words.

In contrast, we found that CP-knowers significantly outperformed subset knowers on the Next Number Task ($\beta =$ `r round(cp.nn.2$coefficients[2,1], 2)`, $p <$ .0001; \emph{M} = `r round(means_overall_2.nn$mean_correct[1],2)`), and critically, that there was no difference in CP-knowers' Unit Task and Next Number performance ($\beta =$ `r round(cp.nn.sum$coefficients[2,1], 2)`, $p =$ .21), as shown in Figure \ref{fig:exp_2_cp_sub}. Thus, we did find some evidence that CP-knowers who have access to the ordinal structure of the count list may be able to recruit this knowledge when reasoning about successor relations for these small numbers. However, the fact that CP-knowers were nevertheless at chance for sets of 4 and 5 in the Unit Task suggests that not all children were capable of making the analogical mapping required in this task.

```{r exp_2_unit_nn, fig.pos = "tb", fig.width=3.5, fig.height=1.9, fig.align = "center", fig.cap = "Mean Unit Task and Next Number performance for subset knowers in Exp. 2, grouped by knower level. Error bars represent 95\\% CIs computed by nonparametric bootstrap. Three-, 4-, and 5- knowers shown together."}
task.pal <- c("#cb4b16", "#2aa198")

all.data.study2 %>%
   filter(Task == "SF" | 
           Task == "Next_number")%>%
  filter(Knower_level == "1" | 
           Knower_level == "2" | 
           Knower_level == "3" | 
           Knower_level == "4" | 
           Knower_level == "5")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Knower_level.combined = ifelse((Knower_level == "4" | Knower_level == "3" | Knower_level == "5"), "3-, 4-, & 5-knowers", as.character(Knower_level)),
         Knower_level.combined = factor(Knower_level.combined, levels= c("1", "2", "3-, 4-, & 5-knowers"), 
                               labels = c("1-knowers", "2-knowers", "3-, 4-, & 5-knowers")), 
         Task = factor(Task, levels = c("Next_number", "SF"), 
                labels = c("Next Number", "Unit Task")))%>%
  group_by(Knower_level.combined, Task, Task_item)%>%
  # summarise(n = n(), 
  #           mean = mean(Correct, na.rm = TRUE), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Task, group= Task)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size = 7.5) + 
  theme(legend.position = "top", 
        panel.grid = element_blank(), 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  facet_wrap(~Knower_level.combined) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(0, 1)) + 
  scale_color_manual(values = task.pal)+ 
  labs(x = "Number queried", y = "Mean task performance")
```

```{r exp_2_cp_sub, fig.width=3.5, fig.height=1.9, fig.align = "center", fig.cap = "Mean Unit Task and Next Number performance for CP- and subset knowers in Exp. 2. Error bars represent 95\\% CIs computed by nonparametric bootstrap."}
all.data.study2 %>%
   filter(Task == "SF" | 
           Task == "Next_number")%>%
  mutate(Task_item = factor(Task_item), 
         Correct = as.numeric(as.character(Correct)), 
         Task = factor(Task, levels = c("Next_number", "SF"), 
                labels = c("Next Number", "Unit Task")), 
         CP_subset = factor(CP_subset, levels = c("Subset", "CP"), 
                            labels = c("Subset-knower", "CP-knower")))%>%
  group_by(CP_subset, Task, Task_item)%>%
  # summarise(n = n(), 
  #           mean = mean(Correct, na.rm = TRUE), 
  #           sd = sd(Correct, na.rm = TRUE), 
  #           se = sd/sqrt(n)) %>%
  langcog::multi_boot_standard("Correct", na.rm = TRUE) %>%
  ggplot(aes(x = factor(Task_item), y = mean, colour = Task, group= Task)) +
  geom_point(size = 1) + 
  geom_line(size = .5) +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), 
                width = 0, size = .5) +
  theme_bw(base_size =7.5) + 
  theme(
        panel.grid = element_blank(), 
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.margin=margin(1,1,1,1),
        legend.box.margin=margin(-7,-7,-7,-7)) + 
  facet_wrap(~CP_subset) + 
  scale_y_continuous(breaks = seq(0,1,.25), limits = c(0, 1)) + 
  scale_color_manual(values = task.pal)+ 
  labs(x = "Number queried", y = "Mean task performance")
```

# General Discussion
For several years before children are able to reason about the result of adding 1 to any cardinality, they seem to have fragmented knowledge of the successor function, successfully implementing it for some familiar numbers, yet failing for others [@davidson2012; @cheung2017; @spaepen2018]. When does this item-based knowledge arise, and how is it represented? We explored these questions in the current work in two ways. In Experiment 1 we asked whether this item-based knowledge occurs only after CP acquisition, or whether subset knowers' previous failures could be explained by the fact that they are typically tested on numbers that fall outside their known number range. In Experiment 2, we explored the possible mechanisms underlying the origins of children's implicit successor knowledge, testing whether it was dependent on knowledge of the count list, or could be explained by mappings between known number words and sets stored in working memory. 

In both Experiments, we found that subset knowers performed significantly better than chance on the Unit Task when tested on items that fell within their known number range, even though they could almost always count beyond their Knower-level. However, we did not find evidence that subset knowers' Unit Task performance was related to their Highest Count, suggesting that children were not using the count list to succeed in this task. We investigated this further in Experiment 2 by comparing children's performance on the Unit Task to the Next Number task, a more rigorous test of count list knowledge. Compatible with the hypothesis that children could be establishing this item-based knowledge through mapping directly from representations of sets to known number words, rather than reasoning about relations between number words, we found that subset knowers' Unit Task performance was unrelated to their Next Number performance. These results suggest that children may begin to acquire item-based successor mappings quite early in numerical development, and that these mappings may be initially independent of count list knowledge.

Consistent with other work, however, we found that children’s successor knowledge increased after they acquired the CP, with CP-knowers out-performing subset knowers on the Unit Task in both Experiments. Further, we found some evidence that some CP-knowers, who have some understanding of the count list and its relationship to cardinality, may actively recruit this knowledge in establishing these item-based successor mappings. Although we did not find a relationship between CP-knowers' Highest Count and Unit Task performance in Experiments 1 and 2, in Experiment 2 we found that CP-knowers' Next Number performance was not significantly different than their Unit Task performance. While recent work has suggested that children eventually use the structure of the count list to make a full induction about the successor function [@cheung2017;@chu2020], our results indicate that even young CP-knowers may be starting to recognize the count list’s significance in solving these tasks. That is, some CP-knowers may know that adding 1 to "four" results in a set of "five", or that adding 1 to "seven" yields "eight," but that they haven't made the inductive inference that for all numbers, \emph{N}, adding 1 results in a set labeled by the successor of \emph{N}, \emph{N}+1. CP-knowers' performance on sets that fell outside the limits of working memory (4 and 5) and could only have been solved by using the count list indicates that this knowledge is not necessarily entailed by acquiring the CP, however. 

What role do these early item-based successor mappings play in leading children to the eventual conclusion that every number has a successor? One possibility is that these early item-based mappings may play an important role in establishing the basic set-operation mechanisms needed to implement the successor function. Our results suggest that this process may begin even before children learn the significance of the count routine, and is constrained both by their number word knowledge, but also by the mechanism that supports these mappings. These data speak to a key question in children's acquisition of successor knowledge, which is why many otherwise competent counters nevertheless exhibit such limited performance in the Unit Task. Here, we find evidence that the item-based knowledge children exhibit at this point in numerical development may not be relational, but may instead be based on mappings made directly between sets and known number words. Thus, while the eventual inductive inference that children must make -- that successive numbers in the count list each denote a +1 increase in cardinality -- is relational, and requires item-based relational mappings as inputs, our data suggest that children do not begin to learn these relational mappings at least until they have acquired the CP. After children become CP-knowers, and their knowledge of the count list grows, the set of numbers for which they are able to reason about these mappings may increase, causing children to assert that it is possible to add 1 to some, but not all numbers. These localized networks may lay the ground for a larger induction about the successor function, as proposed by @davidson2012. This induction may be made when children learn that number words are recursively generated — and that the item-based successor function that they apply to some numbers actually extends to all numbers [@cheung2017;@chu2020;@schneider2020]. 

Taken together, our results suggest that children begin establishing the general conceptual framework which grounds successor function knowledge quite early in development, and is not dependent upon knowing the CP. Prior to learning the significance of the count routine and its relationship to cardinality, children use the numerical tools at their disposal, such as mappings between sets, known number words, and (eventually) limited portions of the count list, to begin establishing the basic set-operations of the successor function. As children acquire more numerical tools and knowledge, they are gradually incorporated into this conceptual framework, possibly forming the foundation for a larger induction about this logical principle. 
<!-- From intro: While much remains to be discovered about children's successor function acquisition, a potential framework for this process has begun to emerge. It is possible that children first begin learning about the successor function by establishing a network of localized successor relations, as proposed by @davidson2012. As children master more of the the count list, these new numbers may be gradually incorporated into this network, incrementally providing evidence that adding +1 to cardinalities results in a +1 change in the count list. Mastering the recursive structure of the count list may allow children to extend this item-based knowledge, supporting an induction about how the successor function is implicated in the recursive generation of number words [@schneider2020;@chu2020]. -->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
